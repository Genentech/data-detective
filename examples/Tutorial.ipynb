{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961c370f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d45ce",
   "metadata": {},
   "source": [
    "In this tutorial, we will explain the basics of using Data Detective to perform a Data Investigation. In this tutorial, we will go through the steps of setting up and running a basic Data Detective Investigation, including: \n",
    "1. Configuring a dataset that works with Data Detective. \n",
    "2. Setting up a schema that defines the Data Detective investigation. \n",
    "3. Executing a data detective investigation. \n",
    "4. Summarizing results using the built-in Rank Aggregator\n",
    "\n",
    "\n",
    "We will also include supplemental tutorials for some of Data Detective's more advanced features, including: \n",
    "- Extending the Data Detective investigation with custom validations\n",
    "- Extending the transform library to map custom datatypes to supported datatypes\n",
    "\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1139eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.pardir, '.'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "\n",
    "from src.actions.action import RemoveTopKAnomalousSamplesAction\n",
    "from src.aggregation.rankings import ResultAggregator, RankingAggregationMethod, ScoreAggregationMethod\n",
    "from src.data_detective_engine import DataDetectiveEngine\n",
    "from src.datasets.tutorial_dataset import TutorialDataset\n",
    "from src.datasets.data_detective_dataset import dd_random_split\n",
    "\n",
    "%pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfa30b",
   "metadata": {},
   "source": [
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028623e",
   "metadata": {},
   "source": [
    "## Requirements for a Dataset\n",
    "\n",
    "For a dataset to work within the Data Detective framework, it needs to satisfy the following requirements: \n",
    "\n",
    "1. It must override the `__getitem__` method that returns a dictionary mapping from each data column key to the data value. \n",
    "2. It must contain a `datatypes` method that returns a dictionary mapping from each data column key to the column's datatype. \n",
    "\n",
    "Let's examine what this looks like in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87d144",
   "metadata": {},
   "source": [
    "## Dataset Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd5898",
   "metadata": {},
   "source": [
    "In this tutorial, we will create a heterogeneous dataset that consists of the following items: \n",
    "\n",
    "- MNIST images\n",
    "- MNIST labels\n",
    "- 10-dimensional normal distribution (μ=0, σ=1)\n",
    "\n",
    "The full dataset can be found under src/datasets/tutorial_dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TutorialDataset(\n",
    "    root='./data/MNIST',\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor() \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7faeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show_datapoint(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea598dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"normal_vector_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a420b",
   "metadata": {},
   "source": [
    "# Setting up the Data Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87def2d",
   "metadata": {},
   "source": [
    "The *data object* is a dictionary that consists of the preprocessed dataset and (optionally) its splits. In order to make use of split validation techniques such as distribution shift between splits, the data splits must be included. \n",
    "\n",
    "There are 3 basic categories of split objects that can be included in the data object:\n",
    "1. **The Entire Set**. This set consists of the entire dataset being analyzed. This is held under the key `entire_set`.\n",
    "2. **Unordered Split Sets**. These are splits that are order-invariant for the purposes of analysis. For example, when doing covariate analysis between splits, the order and names of the dataset does not need to be specified to compare all datasets to all other datasets. They are defined as a nested dictionary under the key `split_group_set` under the key `split_group_set`; see the example below for more details. \n",
    "3. **Ordered Splits**. These are splits that are order-dependent for the purposes of analysis. For example, when doing OOD detection, it is important to define a source dataset for training the OOD model and the target split for evaluation. \n",
    "   - Source/training splits are labeled as the `everything_but_inference_set`\n",
    "   - Target/inference splits are labeled as the `inference_set`\n",
    "\n",
    "The example below should clarify these distinctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_size: int = 20\n",
    "everything_but_inference_size: int = dataset.__len__() - inference_size\n",
    "inference_dataset, everything_but_inference_dataset = dd_random_split(dataset, [inference_size, dataset.__len__() - inference_size])\n",
    "    \n",
    "train_size: int = int(0.6 * len(everything_but_inference_dataset))\n",
    "val_size: int = int(0.2 * len(everything_but_inference_dataset))\n",
    "test_size: int = len(everything_but_inference_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = dd_random_split(everything_but_inference_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "data_object = {\n",
    "    \"entire_set\": dataset,\n",
    "    \"everything_but_inference_set\": everything_but_inference_dataset,\n",
    "    \"inference_set\": inference_dataset,\n",
    "    \"train/val/test\": {\n",
    "        \"training_set\": train_dataset,\n",
    "        \"validation_set\": val_dataset,\n",
    "        \"test_set\": test_dataset,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"size of inference_dataset: {inference_dataset.__len__()}\")\n",
    "print(f\"size of everything_but_inference_dataset: {everything_but_inference_dataset.__len__()}\")\n",
    "print(f\"size of train_dataset: {train_dataset.__len__()}\")\n",
    "print(f\"size of entire dataset: {dataset.__len__()}\")\n",
    "print(f\"size of val_dataset: {val_dataset.__len__()}\")\n",
    "print(f\"size of test_dataset: {test_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2197dc",
   "metadata": {},
   "source": [
    "# Setting up a Validation Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be22c1",
   "metadata": {},
   "source": [
    "## Specifying Validators and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033c9e0",
   "metadata": {},
   "source": [
    "The validation schema contains information about the types of checks that will be executed by the Data Detective Engine and the transforms that Data Detective will use. Before discussing the validation schema, it is important to define two key modular components that make up Data Detective investigations. Data Detective's functionality can be divided into a modular, implementation-heavy component referred to as a *validator method* and an easily toggleable component referred to as a *validator*.\n",
    "\n",
    "A *validator method* performs a specific type of test for a data issue on a specific data type. These validator methods primarily consist of the code needed to take a dataset and run an evaluation on it that produces either a positive or negative result or a score. Some examples of validator methods include:\n",
    "- Mann-Whitney U-Test to examine distribution shift between train/test splits on tabular data\n",
    "- Kernel Conditional Independence (KCI) test for validating causal assumptions on vectorvalued\n",
    "data\n",
    "- Isolation forest trained/evaluated on image histograms for identifying anomalies in imaging\n",
    "data\n",
    "\n",
    "*Validations* are toggleable, datatype-agnostic collections of validator methods that are serially applied to the dataset. Each validator targets a single problem that may arise in data, including: \n",
    "- Shift between different data splits\n",
    "- Outlier and anomaly detection\n",
    "- Violation of parametric assumptions on the data\n",
    "- Violation of expected casual structures / conditional independences in the \n",
    "\n",
    "In the validation schema, users only specify the validators that they would like to use. This abstracts away details concerning what methods should be used for which columns and simplifies the process for searching for a particular flavor of issues to a few lines of code. Below is the validation schema that we will use for the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_schema : Dict = {\n",
    "    \"validators\": {\n",
    "        \"unsupervised_anomaly_data_validator\": {\n",
    "            \"validator_kwargs\":{\n",
    "                \"should_return_model_instance\": True\n",
    "            }\n",
    "        },\n",
    "        # \"unsupervised_multimodal_anomaly_data_validator\": {},\n",
    "        # \"split_covariate_data_validator\": {},\n",
    "        # \"ood_inference_data_validator\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5ebfa",
   "metadata": {},
   "source": [
    "A few notes: \n",
    "- Each validator maps to an object that specifies additional options. For this tutorial, we will use the default settings, but these options include: \n",
    "  - special kwargs to include and pass to the validator methods\n",
    "  - additional filtering regarding which columns the validator should be applied to\n",
    " \n",
    "  \n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f89af",
   "metadata": {},
   "source": [
    "## Specifying Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1525d",
   "metadata": {},
   "source": [
    "It may be the case that you are using a data modality that has little to no method infrastructure in Data Detective. The simplest way to make use of all of Data Detective's functionality is to use a transform that maps this data modality to a well-supported modality in Data Detective such as multidimensional data. In our example, we will be making use of a pretrained resnet50 backbone to map our MNIST images to 2048 dimensional vectors. This will allow us to make use of methods built for multidimensional data on our MNIST image representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_schema : Dict = {\n",
    "    \"transforms\": {\n",
    "        \"IMAGE\": [{\n",
    "            \"name\": \"resnet50\",\n",
    "            \"in_place\": \"False\",\n",
    "            \"options\": {},\n",
    "        }],\n",
    "    }\n",
    "}\n",
    "     \n",
    "full_validation_schema: Dict = {\n",
    "    **validation_schema, \n",
    "    **transform_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3611d3",
   "metadata": {},
   "source": [
    "# Running the Data Detective Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecb162",
   "metadata": {},
   "source": [
    "Now that the full validation schema and data object are prepared, we are ready to run the Data Detective Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_detective_engine = DataDetectiveEngine()\n",
    "\n",
    "start_time = time.time()\n",
    "results = data_detective_engine.validate_from_schema(full_validation_schema, data_object, run_concurrently=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dca144",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e1d58",
   "metadata": {},
   "source": [
    "Great! Let's start to look at and analyze the results we've collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84578eae",
   "metadata": {},
   "source": [
    "# Aggregating + Examining Results using the Built-In Result Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = ResultAggregator(results_object=results)\n",
    "modal_rankings = aggregator.aggregate_results_modally(\"unsupervised_anomaly_data_validator\", [RankingAggregationMethod.LOWEST_RANK, RankingAggregationMethod.HIGHEST_RANK, RankingAggregationMethod.ROUND_ROBIN], given_data_modality=\"resnet50_backbone_mnist_image\")\n",
    "total_rankings = aggregator.aggregate_results_multimodally(\"unsupervised_anomaly_data_validator\", [RankingAggregationMethod.LOWEST_RANK, RankingAggregationMethod.HIGHEST_RANK, RankingAggregationMethod.ROUND_ROBIN, ScoreAggregationMethod.NORMALIZED_AVERAGE])\n",
    "# rankings = total_rankings\n",
    "rankings = modal_rankings\n",
    "\n",
    "rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffcca37",
   "metadata": {},
   "source": [
    "# Taking Action: Fixing our Dataset with Built-in Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebe5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = RemoveTopKAnomalousSamplesAction()\n",
    "new_data_object = action.get_new_data_object(\n",
    "    data_object,\n",
    "    rankings, \n",
    "    \"round_robin_agg_rank\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in {'114', '286', '28', '81', '153', '333', '53', '453', '12', '10'}:\n",
    "    dataset.show_datapoint(int(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551915b",
   "metadata": {},
   "source": [
    "# Wait, but Why: Occlusion-Oriented Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d544f",
   "metadata": {},
   "source": [
    "Lets look at a couple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe956df",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICES_TO_EXAMINE = [98, 10, 12]\n",
    "\n",
    "from src.transforms.transform_library import TRANSFORM_LIBRARY\n",
    "from src.utils import OcclusionTransform, occlusion_interpretability\n",
    "\n",
    "METHOD = 'cblof_anomaly_validator_method'\n",
    "\n",
    "resnet = TRANSFORM_LIBRARY['resnet50']()\n",
    "resnet.initialize_transform(transform_kwargs={})\n",
    "print(resnet)\n",
    "\n",
    "for idx in INDICES_TO_EXAMINE:\n",
    "    sample = dataset[idx]\n",
    "    img = sample['mnist_image']\n",
    "    img = img.reshape((1, 28, 28))\n",
    "    occ = OcclusionTransform(width=5)\n",
    "    occed = occ(img, (15, 15))\n",
    "\n",
    "    model_results, model = results['unsupervised_anomaly_data_validator'][METHOD]['resnet50_backbone_mnist_image_results']\n",
    "    res_min, res_max = min(model_results.values()), max(model_results.values())\n",
    "    interp_results = occlusion_interpretability(img, model, occ, (res_min, res_max))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
