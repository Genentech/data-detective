{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961c370f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d45ce",
   "metadata": {},
   "source": [
    "In this tutorial, we will explain the basics of extending Data Detective to new data types, new validator methods, new validators, and new transforms. We will build up to performing anomaly detection on the MNIST dataset using PCA anomaly scoring.\n",
    "\n",
    "Prerequisites include all of the information in the Data Detective Basics tutorial.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pyod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1139eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pyod\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.pardir, '.'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from typing import Dict, Set, Type\n",
    "\n",
    "from src.aggregation.rankings import ResultAggregator, RankingAggregationMethod\n",
    "from src.data_detective_engine import DataDetectiveEngine\n",
    "from src.enums.enums import DataType, ValidatorMethodParameter\n",
    "from src.validators.data_validator import DataValidator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfa30b",
   "metadata": {},
   "source": [
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.tutorial_dataset import TutorialDataset\n",
    "\n",
    "\n",
    "dataset = TutorialDataset(\n",
    "    root='./data/MNIST',\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a420b",
   "metadata": {},
   "source": [
    "# Creating a Validator Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b079a2",
   "metadata": {},
   "source": [
    "## The Structure of Validator Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87def2d",
   "metadata": {},
   "source": [
    "Each validator method is a static class that has 4 static, functional methods:\n",
    "1. The `datatype` method, which returns a set of datatypes that are supported by the validator method. \n",
    "<!---The set is considered an \"OR\" relation (that is, if any of of the datatypes in the set are present in the dataset, the validator method will be applied). --->\n",
    "2. The `param_keys` method, which returns a set containinng the data splits that the method applies to. \n",
    "3. The `validate` method, which returns some type of actionable result. \n",
    "4. The `get_method_kwargs` method, which takes in the data object and validator kwargs and sets up the calls to `validate`. \n",
    "\n",
    "Let's go through a simple validator method and examine all of these components. The example validator method that we will be examining determines the principle components of a multidimensional column over a dataset and uses reconstruction error over the fitted principle components to provide an anomaly score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcf64b",
   "metadata": {},
   "source": [
    "## Example: PCA Anomaly Validator Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78378f",
   "metadata": {},
   "source": [
    "The first step in writing a new validator method is creating a good test for the validator method using synthetic data. Tests are a crucial part of the data detective validator method construction process for three reasons: \n",
    "1. They are helpful early on in the design process for considering and enforcing sensible top-down interface decisions.\n",
    "2. They are a useful piece of documentation to both yourself as you write the method and to an end user in understanding how to use your method.\n",
    "3. They verify correctness of your implementation. \n",
    "\n",
    "For our example, we will be constructing a 10-dimensional synthetic normal dataset with 99% of samples drawn from N(μ=0, σ=1) and 1% of samples drawn from N(μ=10, σ=1). In order to examine correctness, we will look at the AUCRoC scores between the true anomaly labels and the incorrect anomaly labels. The test that we will be using is shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa3941",
   "metadata": {},
   "source": [
    "### 1. `datatype()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37e536",
   "metadata": {},
   "source": [
    "We would like our PCA method to take in only multidimensional data, so let's specify that in the `datatype()` method. We specify this by returning a set of `DataType` objects. If we had wanted to start extending support to new datatypes, we would at this point extend the `DataType` enumeration and specify the new data type in the `datatype` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datatype() -> Set[DataType]:\n",
    "    \"\"\"\n",
    "    @return: the datatype the validators method operates on.\n",
    "    \"\"\"\n",
    "    return { DataType.MULTIDIMENSIONAL }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f08db",
   "metadata": {},
   "source": [
    "### 2. `param_keys()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7085b",
   "metadata": {},
   "source": [
    "PCA Anomaly validation is an unsupervised method, so it needs to take in the entire dataset to fit/evaluate the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_keys() -> Set[ValidatorMethodParameter]:\n",
    "    \"\"\"\n",
    "    Lists the data splits that the validators operates on.\n",
    "    \n",
    "    @return: a set of data splits for the .validate() method.\n",
    "    \"\"\"\n",
    "    return { ValidatorMethodParameter.ENTIRE_SET }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc5345",
   "metadata": {},
   "source": [
    "### 3. `validate()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1fc7a",
   "metadata": {},
   "source": [
    "Our `validate()` method will map us from some representation of the data to a single result. For the PCA `validate()` method, let's choose to take in the entire n x d data matrix for a given column of data and an option indicating the number of components to keep for computation of outlier scores. In the method body, we will fit an existing PCA anomaly detection method from `pyod` and use that model to give us a set of anomaly scores based on reconstruction loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75981c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.pca import *\n",
    "\n",
    "def validate(\n",
    "    data_matrix: Type[np.array] = None, # n x d data matrix for a givenn column\n",
    "    n_components=None,\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    Runs PCA anomaly detection.\n",
    "\n",
    "    @return: a list of n scores, one per sample. \n",
    "    \"\"\"\n",
    "    model = pyod.models.pca.PCA(n_components=n_components)\n",
    "    model.fit(data_matrix)\n",
    "\n",
    "    anomaly_scores = model.decision_function(data_matrix)\n",
    "\n",
    "    # mapping output results to sample ids.\n",
    "    return dict(enumerate(anomaly_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43695655",
   "metadata": {},
   "source": [
    "### 4. `get_method_kwargs()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5a03d",
   "metadata": {},
   "source": [
    "In the `get_method_kwargs()` method, we will be taking the set of options passed in the validation schema as well as the data object and setting up the calls to the `validate()` method. This method should return a dictionary where each value contains the kwargs for a `validate()` call and each key reflects where the `validate()` call will store its results in the final method results dictionary. \n",
    "\n",
    "Every `get_method_kwargs()` method accepts two things: the validation schema and the data object. For our PCA anomaly example, we will want to perform one call for each entry in the data object, giving us a score for each column of each sample. \n",
    "\n",
    "It is helpful to know for this particular method setup that every dataset contains a `get_matrix` attribute that can be overridden and further optimized to retrieve a matrix representation of the Data Detective Dataset without iteration or dataloading. This is especially useful for working with parquet or CSV data when you need to optimize your setup for performance over flexibility.\n",
    "\n",
    "<!--Every `get_method_kwargs()` method accepts two things: the validation schema and the (filtered) data object. The data object is preliminarily filtered in two ways: \n",
    "1. The `include` option in the validation schema accepts a list of regular expressions under each \n",
    "2. The `datatype()` method results in the data object being filtered to only include columns in that data object.-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e81dd2",
   "metadata": {},
   "source": [
    "Let's write our `get_method_kwargs()` function, which needs to retrieve our `data_matrix` and `n_components` parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_method_kwargs(data_object: Dict[str, torch.utils.data.Dataset], validator_kwargs: Dict = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Gets the arguments for each run of the validator_method, and what to store the results under.\n",
    "\n",
    "    @param data_object: the datasets object containing the datasets (train, test, entire, etc.)\n",
    "    @param validator_kwargs: the kwargs from the validation schema.\n",
    "    @return: a dict mapping from the key the result from calling .validate() on the kwargs values.\n",
    "    \"\"\"\n",
    "    entire_dataset: torch.utils.data.Dataset = data_object[\"entire_set\"]\n",
    "    matrix_dict = entire_dataset.get_matrix(column_wise=True)\n",
    "        \n",
    "    kwargs_dict = {\n",
    "        f\"{column}_results\": {\n",
    "            \"data_matrix\": column_data,\n",
    "            \"n_components\": validator_kwargs.get(\"n_components\")\n",
    "                            # ^will default to None if n_components is not provided\n",
    "        } for column, column_data in matrix_dict.items()\n",
    "    }\n",
    "\n",
    "    return kwargs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793b317",
   "metadata": {},
   "source": [
    "Great! Let's wrap all of the methods we have written in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Set, Dict, Type, Union\n",
    "\n",
    "from src.datasets.data_detective_dataset import DataDetectiveDataset\n",
    "from src.enums.enums import DataType, ValidatorMethodParameter\n",
    "from src.validator_methods.data_validator_method import DataValidatorMethod\n",
    "\n",
    "class MyPCAAnomalyValidatorMethod(DataValidatorMethod):\n",
    "    \"\"\"\n",
    "    A method for determining anomalies on multidimensional data. Operates on continuous datasets.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def name() -> str: \n",
    "        return \"my_pca_validator_method\"\n",
    "\n",
    "    @staticmethod\n",
    "    def datatype() -> Set[DataType]:\n",
    "        return datatype()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def param_keys() -> Set[ValidatorMethodParameter]:\n",
    "        \"\"\"\n",
    "        @return: a set of data splits for the data object to include.\n",
    "        \"\"\"\n",
    "        return param_keys()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_method_kwargs(data_object: Dict[str, Union[DataDetectiveDataset, Dict[str, DataDetectiveDataset]]], validator_kwargs: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Gets the arguments for each run of the validator_method, and what to store the results under.\n",
    "\n",
    "        @param data_object: the datasets object containing the datasets (train, test, entire, etc.)\n",
    "        @param validator_kwargs: the kwargs from the validation schema.\n",
    "        @return: a dict mapping from the key the result from calling .validate() on the kwargs values.\n",
    "        \"\"\"\n",
    "        return get_method_kwargs(data_object, validator_kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def validate(\n",
    "        data_matrix: Type[np.array] = None, # n x d data matrix for a givenn column\n",
    "        n_components=None,\n",
    "    ) -> object:\n",
    "        \"\"\"\n",
    "        Runs PCA anomaly detection.\n",
    "\n",
    "        @return: a list of n scores, one per sample. \n",
    "        \"\"\"\n",
    "        return validate(data_matrix, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2197dc",
   "metadata": {},
   "source": [
    "# Creating a Validator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be22c1",
   "metadata": {},
   "source": [
    "Validators are simply sets of validator methods. Creating a new one is relatively straightforward. They consist of the set of validator methods that they include. Let's create a non-default validator for our PCA anomaly method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyUnsupervisedAnomalyDataValidator(DataValidator):\n",
    "    \"\"\"\n",
    "    A dataset has many features/columns, and each column has many ValidatorMethods that apply to it, depending on the\n",
    "    datatype. A DataValidator is a collection of ValidatorMethods for a unique purpose.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def name() -> str: \n",
    "        return \"my_unsupervised_anomaly_data_validator\"\n",
    "\n",
    "    @staticmethod\n",
    "    def validator_methods() -> Set[Type[DataValidatorMethod]]:\n",
    "        return {\n",
    "            MyPCAAnomalyValidatorMethod\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb84670",
   "metadata": {},
   "source": [
    "# Creating a Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1525d",
   "metadata": {},
   "source": [
    "There are two steps to creating a new transform: \n",
    "\n",
    "1. Creating a new Transform class and overriding appropriate methods.\n",
    "2. Registering the new transform in the transform library. \n",
    "\n",
    "Let's look at an example of a simple transform that maps images to their histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transforms.embedding_transformer import Transform\n",
    "\n",
    "\n",
    "class MyHistogramTransform(Transform): \n",
    "    def __init__(self, in_place: bool = False, cache_values: bool = True):        \n",
    "        super().__init__(\n",
    "            new_column_datatype=DataType.MULTIDIMENSIONAL,\n",
    "            in_place=in_place, \n",
    "            cache_values=cache_values\n",
    "        )\n",
    "    \n",
    "    def initialize_transform(self, transform_kwargs):\n",
    "        super().initialize_transform(transform_kwargs=transform_kwargs)\n",
    "\n",
    "        if \"data_object\" in transform_kwargs.keys():\n",
    "            transform_kwargs.pop(\"data_object\")\n",
    "        if \"column\" in transform_kwargs.keys():\n",
    "            transform_kwargs.pop(\"column\")\n",
    "\n",
    "        self.num_bins = transform_kwargs.get(\"bins\", 10)\n",
    "\n",
    "    def transform(self, x):\n",
    "        x_norm = (x - x.min()) / (x.max() - x.min())\n",
    "        return torch.FloatTensor(np.histogram(x_norm, bins=self.num_bins)[0])\n",
    "\n",
    "    def new_column_name(self, original_name): \n",
    "        return f\"histogram_{original_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b359c",
   "metadata": {},
   "source": [
    "There are a few patterns worth noting in the above implementation. The `get_{transform}` higher order function always takes in kwargs that are passed through from the `options` parameter of the input transforms. \n",
    "\n",
    "The most important is the use of an inner helper function (in this case, `full_impl`) that is returned. Returning an inner function allows for one-time initialization of the backbone and of the parsing of options in kwargs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05928cc",
   "metadata": {},
   "source": [
    "Now that we have our new validator and transform, let's register them with the Data Detective Engine so we can use them in our investigation and try them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_detective_engine = DataDetectiveEngine()\n",
    "\n",
    "data_detective_engine.register_validator(MyUnsupervisedAnomalyDataValidator)\n",
    "data_detective_engine.register_transform(MyHistogramTransform, \"my_histogram\")\n",
    "\n",
    "validation_schema = {\n",
    "    \"validators\": {\n",
    "        \"my_unsupervised_anomaly_data_validator\": {},\n",
    "    },\n",
    "    \"transforms\": {\n",
    "        \"IMAGE\": [{\n",
    "            \"name\": \"my_histogram\",\n",
    "            \"in_place\": \"False\",\n",
    "            \"options\": {},\n",
    "        }],\n",
    "    }\n",
    "}\n",
    "\n",
    "data_object = {\n",
    "    \"entire_set\": dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3611d3",
   "metadata": {},
   "source": [
    "# Running the Data Detective Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecb162",
   "metadata": {},
   "source": [
    "Now that the full validation schema and data object are prepared, we are ready to run the Data Detective Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57946fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = data_detective_engine.validate_from_schema(validation_schema, data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e1d58",
   "metadata": {},
   "source": [
    "Great! Let's start to look at and analyze the results we've collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84578eae",
   "metadata": {},
   "source": [
    "# Interpreting Results using the Built-In Rank Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0cbba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggregator = ResultAggregator(results_object=results)\n",
    "input_df = aggregator.aggregate_results_multimodally(\"my_unsupervised_anomaly_data_validator\", [RankingAggregationMethod.LOWEST_RANK, RankingAggregationMethod.HIGHEST_RANK, RankingAggregationMethod.ROUND_ROBIN])\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show_datapoint(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
