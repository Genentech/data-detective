{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961c370f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d45ce",
   "metadata": {},
   "source": [
    "In this tutorial, we will explain the basics of using Data Detective to perform a Data Investigation. In this tutorial, we will go through the steps of setting up and running a basic Data Detective Investigation, including: \n",
    "1. Configuring a dataset that works with Data Detective. \n",
    "2. Setting up a schema that defines the Data Detective investigation. \n",
    "3. Executing a data detective investigation. \n",
    "4. Summarizing results using the built-in Rank Aggregator\n",
    "\n",
    "\n",
    "We will also include supplemental tutorials for some of Data Detective's more advanced features, including: \n",
    "- Extending the Data Detective investigation with custom validations\n",
    "- Extending the transform library to map custom datatypes to supported datatypes\n",
    "\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1139eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from typing import Dict, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.pardir, '.'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from constants import FloatTensor\n",
    "from src.aggregation.rankings import ResultAggregator, RankingAggregationMethod\n",
    "from src.data_detective_engine import DataDetectiveEngine\n",
    "from src.datasets.data_detective_dataset import dd_random_split\n",
    "from src.datasets.my_cifar_10 import MyCIFAR10\n",
    "from src.datasets.my_fashion_mnist import MyFashionMNIST\n",
    "from src.enums.enums import DataType\n",
    "from src.transforms.embedding_transformer import Transform\n",
    "from src.transforms.transform_library import TRANSFORM_LIBRARY\n",
    "\n",
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: MyFashionMNIST = MyFashionMNIST(\n",
    "    root='./datasets/FashionMNIST',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "percent_to_keep = 0.01\n",
    "dataset, _ = dd_random_split(dataset, [int(dataset.__len__() * percent_to_keep), dataset.__len__() - int(dataset.__len__() * percent_to_keep)])\n",
    "print(dataset.__len__())\n",
    "print(_.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_size: int = 20\n",
    "everything_but_inference_size: int = dataset.__len__() - inference_size\n",
    "inference_dataset, everything_but_inference_dataset = dd_random_split(dataset, [inference_size, dataset.__len__() - inference_size])\n",
    "    \n",
    "train_size: int = int(0.6 * len(everything_but_inference_dataset))\n",
    "val_size: int = int(0.2 * len(everything_but_inference_dataset))\n",
    "test_size: int = len(everything_but_inference_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = dd_random_split(everything_but_inference_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "data_object: Dict[str, torch.utils.data.Dataset] = {\n",
    "    \"entire_set\": dataset,\n",
    "    \"everything_but_inference_set\": everything_but_inference_dataset,\n",
    "    \"inference_set\": inference_dataset,\n",
    "    \"train/val/test\":{\n",
    "        \"training_set\": train_dataset,\n",
    "        \"validation_set\": val_dataset,\n",
    "        \"test_set\": test_dataset,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"size of inference_dataset: {inference_dataset.__len__()}\")\n",
    "print(f\"size of everything_but_inference_dataset: {everything_but_inference_dataset.__len__()}\")\n",
    "print(f\"size of train_dataset: {train_dataset.__len__()}\")\n",
    "print(f\"size of entire dataset: {dataset.__len__()}\")\n",
    "print(f\"size of val_dataset: {val_dataset.__len__()}\")\n",
    "print(f\"size of test_dataset: {test_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2197dc",
   "metadata": {},
   "source": [
    "# Setting up a Validation Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_schema : Dict = {\n",
    "    \"validators\": {\n",
    "        \"unsupervised_anomaly_data_validator\": {\n",
    "            \"validator_kwargs\": {\n",
    "                \"should_return_model_instance\": True\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_schema : Dict = {\n",
    "    \"transforms\": {\n",
    "        \"IMAGE\": [{\n",
    "            \"name\": \"resnet50\",\n",
    "            \"in_place\": \"False\",\n",
    "            \"options\": {},\n",
    "        }],\n",
    "    }\n",
    "}\n",
    "    \n",
    "full_validation_schema: Dict = {\n",
    "    **validation_schema, \n",
    "    **transform_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3611d3",
   "metadata": {},
   "source": [
    "# Running the Data Detective Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ee2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_detective_engine = DataDetectiveEngine()\n",
    "\n",
    "start_time = time.time()\n",
    "results = data_detective_engine.validate_from_schema(full_validation_schema, data_object)\n",
    "\n",
    "# results = data_detective_engine.validate_from_schema(full_validation_schema, data_object)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c321f9",
   "metadata": {},
   "source": [
    "# Setting Up Occlusion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0371c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = TRANSFORM_LIBRARY['resnet50']\n",
    "resnet.initialize_transform({})\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6da321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "from constants import FloatTensor\n",
    "\n",
    "class OcclusionTransform(nn.Module):\n",
    "    def __init__(self, width=5):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        \n",
    "        if width % 2 != 1: \n",
    "            raise Exception(\"Width must be an odd number\")\n",
    "        \n",
    "    def forward(self, tensor, loc): \n",
    "        tensor = copy.deepcopy(tensor)\n",
    "        width = self.width\n",
    "        \n",
    "        diff = (width - 1) / 2\n",
    "        first_dim, second_dim = loc[0], loc[1]\n",
    "        \n",
    "        min_val_first = np.round(max(0, first_dim - diff)).astype(int)\n",
    "        min_val_second = np.round(max(0, second_dim - diff)).astype(int)\n",
    "        \n",
    "        max_val_first = np.round(min(tensor.shape[1], first_dim + diff + 1)).astype(int)\n",
    "        max_val_second = np.round(min(tensor.shape[1], second_dim + diff + 1)).astype(int)\n",
    "        \n",
    "        tensor[:, min_val_first:max_val_first, min_val_second:max_val_second].fill_(0)\n",
    "        \n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_occ_results(img, localized_anomaly_score, width, color_bounds):\n",
    "    if color_bounds != \"auto\":\n",
    "        vmin, vmax = color_bounds\n",
    "    else: \n",
    "        vmin, vmax = None, None\n",
    "    \n",
    "    import pdb; pdb.set_trace()\n",
    "    im = box_blur(localized_anomaly_score, width)\n",
    "    im = im.reshape(im.shape[-2:])\n",
    "    \n",
    "    plt.imshow(img.reshape(img.shape[-2:]), cmap='Greys_r')\n",
    "    plt.colorbar()\n",
    "    plt.suptitle(\"Original Image\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(im, vmin=vmin, vmax=vmax, cmap='plasma')\n",
    "    plt.colorbar()\n",
    "    plt.suptitle(f\"Blurred anomaly occlusion heatmap (width {width})\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(localized_anomaly_score, vmin=vmin, vmax=vmax, cmap='plasma')\n",
    "    plt.colorbar()\n",
    "    plt.suptitle(f\"Unblurred anomaly occlusion heatmap (width {width})\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def box_blur(tensor, width):\n",
    "    def get_sum(sum_table,\n",
    "            min_val_first, \n",
    "            min_val_second, \n",
    "            max_val_first, \n",
    "            max_val_second\n",
    "    ):\n",
    "        x = 0\n",
    "        x += sum_table[max_val_first][max_val_second] \n",
    "\n",
    "        if min_val_second != 0: \n",
    "            x -= sum_table[max_val_first][min_val_second - 1]\n",
    "\n",
    "        if min_val_first != 0:\n",
    "            x -= sum_table[min_val_first - 1][max_val_second] \n",
    "\n",
    "        if not (min_val_first == 0 or min_val_second == 0):\n",
    "            x += sum_table[min_val_first - 1][min_val_second - 1]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_overlaps(tensor, patch_size=(3, 3), patch_stride=(1, 1)):\n",
    "        width = (patch_size[0] - 1) // 2\n",
    "        tensor = FloatTensor(tensor)\n",
    "        tensor = pad(tensor, (width, width, width, width), \"constant\", 0)\n",
    "        while len(tensor.shape) < 4:\n",
    "            tensor = tensor.reshape((-1, *tensor.shape))\n",
    "\n",
    "        n, c, h, w = tensor.size()\n",
    "        px, py = patch_size\n",
    "        sx, sy = patch_stride\n",
    "        nx = ((w-px)//sx)+1\n",
    "        ny = ((h-py)//sy)+1\n",
    "\n",
    "        overlaps = torch.zeros(tensor.size()).type_as(tensor.data)\n",
    "        for i in range(ny):\n",
    "            for j in range(nx):\n",
    "                overlaps[:, :, i*sy:i*sy+py, j*sx:j*sx+px] += 1\n",
    "        overlaps = torch.autograd.Variable(overlaps)\n",
    "        return overlaps[:,:,width:-width,width:-width]\n",
    "\n",
    "    sum_table = tensor.cumsum(axis=0).cumsum(axis=1)\n",
    "\n",
    "    res = np.zeros(tensor.shape)\n",
    "    first_dim_size = tensor.shape[-2]\n",
    "    second_dim_size = tensor.shape[-1]\n",
    "    diff = np.round((width - 1) // 2).astype(int)\n",
    "\n",
    "\n",
    "    for first_dim in range(first_dim_size): \n",
    "        for second_dim in range(second_dim_size): \n",
    "\n",
    "            min_val_first = np.round(max(0, first_dim - diff)).astype(int)\n",
    "            min_val_second = np.round(max(0, second_dim - diff)).astype(int)\n",
    "\n",
    "            max_val_first = np.round(min(tensor.shape[1] - 1, first_dim + diff)).astype(int)\n",
    "            max_val_second = np.round(min(tensor.shape[1] - 1, second_dim + diff)).astype(int)\n",
    "\n",
    "            res[first_dim][second_dim] = get_sum(sum_table,\n",
    "                min_val_first, \n",
    "                min_val_second, \n",
    "                max_val_first, \n",
    "                max_val_second\n",
    "            )\n",
    "\n",
    "    overlap = compute_overlaps(tensor, patch_size=(width, width))\n",
    "        \n",
    "    return res / overlap\n",
    "    \n",
    "    \n",
    "def occlusion_interpretability(img, model, occ, color_bounds=\"auto\"):    \n",
    "    occluded_image_dict = {}\n",
    "    for first_dim in tqdm(range(img.shape[1])):\n",
    "        for second_dim in range(img.shape[2]):\n",
    "            occluded = occ(img, (first_dim, second_dim))\n",
    "            occluded_image_dict[tuple((first_dim, second_dim))] = occluded\n",
    "\n",
    "            \n",
    "    embeddings = np.concatenate([resnet(img) for img in tqdm(occluded_image_dict.values())], axis=0)\n",
    "#     embeddings = np.concatenate(Parallel(n_jobs=6)(delayed(resnet)(img) for img in tqdm(occluded_image_dict.values())), axis=0)\n",
    "    localized_anomaly_scores = model.decision_function(model.normalize(embeddings))\n",
    "    reshaped_localized_anomaly_score = torch.FloatTensor(list(localized_anomaly_scores)).reshape(img.shape[-2:])\n",
    "    plot_occ_results(img, reshaped_localized_anomaly_score, occ.width, color_bounds)\n",
    "    return reshaped_localized_anomaly_score\n",
    "\n",
    "METHOD = 'cblof_anomaly_validator_method'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eeaf1",
   "metadata": {},
   "source": [
    "# Performance on Average (repeat: random) Datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf03756",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[7]\n",
    "img = sample['fashion_mnist_image']\n",
    "occ = OcclusionTransform(width=5)\n",
    "occed = occ(img, (15, 15))\n",
    "\n",
    "model_results, model = results['unsupervised_anomaly_data_validator'][METHOD]['resnet50_backbone_fashion_mnist_image_results']\n",
    "res_min, res_max = min(model_results.values()), max(model_results.values())\n",
    "interp_results = occlusion_interpretability(img, model, occ, (res_min, res_max))\n",
    "\n",
    "# occluded_image_dict = {}\n",
    "# for first_dim in range(img.shape[1]):\n",
    "#     for second_dim in range(img.shape[2]):\n",
    "#         occluded = occ(img, (first_dim, second_dim))\n",
    "#         occluded_image_dict[tuple((first_dim, second_dim))] = occluded\n",
    "\n",
    "# embeddings = np.concatenate([resnet(img) for img in tqdm(occluded_image_dict.values())], axis=0)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8278b",
   "metadata": {},
   "source": [
    "# Most Anomalous Image Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad063267",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_sample_id = max(model_results, key=model_results.get)\n",
    "argmax_sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb191f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results[argmax_sample_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5105f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[argmax_sample_id]['label'])\n",
    "image = dataset[argmax_sample_id]['fashion_mnist_image']\n",
    "# image = sample['image']\n",
    "occ = OcclusionTransform(width=5)\n",
    "occed = occ(image, (15, 15))\n",
    "_, model = results['unsupervised_anomaly_data_validator'][METHOD]['resnet50_backbone_fashion_mnist_image_results']\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image.squeeze())\n",
    "plt.show()\n",
    "plt.imshow(occed.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470e201",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = dataset[argmax_sample_id]\n",
    "img = sample['fashion_mnist_image']\n",
    "occ = OcclusionTransform(width=5)\n",
    "occed = occ(img, (15, 15)) \n",
    "\n",
    "for method in ['iforest', 'cblof', 'pca']:\n",
    "    method_name = f'{method}_anomaly_validator_method'\n",
    "    model_results, model = results['unsupervised_anomaly_data_validator'][method_name]['resnet50_backbone_fashion_mnist_image_results']\n",
    "    res_min, res_max = min(model_results.values()), max(model_results.values())    \n",
    "    interp_results = occlusion_interpretability(img, model, occ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4095fbc",
   "metadata": {},
   "source": [
    "# Added Localized Anomaly Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd776f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[7]\n",
    "img = sample['fashion_mnist_image']\n",
    "\n",
    "img_altered = copy.deepcopy(img)\n",
    "checkerboard = np.indices((7, 7)).sum(axis=0) % 2\n",
    "img_altered[:,:7,:7] = FloatTensor(checkerboard)\n",
    "\n",
    "occ = OcclusionTransform(width=11)\n",
    "# occed = occ(img_altered, (0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = resnet(img)\n",
    "r2 = resnet(img_altered)\n",
    "\n",
    "print(f\"after anomaly, resnet embeddings have a summed abs difference of {np.sum(np.abs(r1 - r2))}\")\n",
    "print(f\"after anomaly, model results have a difference of {model.decision_function(r2) - model.decision_function(r1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd546841",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_altered.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93907dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decision_function(resnet(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e175e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results, model = results['unsupervised_anomaly_data_validator'][METHOD]['resnet50_backbone_fashion_mnist_image_results']\n",
    "res_min, res_max = min(model_results.values()), max(model_results.values())\n",
    "interp_results = occlusion_interpretability(img_altered, model, occ, (res_min, res_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b250f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[7]\n",
    "img = sample['fashion_mnist_image']\n",
    "\n",
    "img_altered = copy.deepcopy(img)\n",
    "checkerboard = np.indices((5, 5)).sum(axis=0) % 2\n",
    "img[:,-5:,-5:] = FloatTensor(checkerboard)\n",
    "\n",
    "occ = OcclusionTransform(width=11)\n",
    "model_results, model = results['unsupervised_anomaly_data_validator'][METHOD]['resnet50_backbone_fashion_mnist_image_results']\n",
    "res_min, res_max = min(model_results.values()), max(model_results.values())\n",
    "interp_results = occlusion_interpretability(img, model, occ, (res_min, res_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70952a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[7]\n",
    "img = sample['fashion_mnist_image']\n",
    "\n",
    "img_altered = copy.deepcopy(img)\n",
    "checkerboard = np.indices((5, 5)).sum(axis=0) % 2\n",
    "img[:,:5,-5:] = FloatTensor(checkerboard)\n",
    "\n",
    "occ = OcclusionTransform(width=11)\n",
    "model_results, model = results['unsupervised_anomaly_data_validator'][METHOD]['resnet50_backbone_fashion_mnist_image_results']\n",
    "res_min, res_max = min(model_results.values()), max(model_results.values())\n",
    "interp_results = occlusion_interpretability(img, model, occ, (res_min, res_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[7]\n",
    "img = sample['fashion_mnist_image']\n",
    "\n",
    "img_altered = copy.deepcopy(img)\n",
    "checkerboard = np.indices((5, 5)).sum(axis=0) % 2\n",
    "img[:,-5:,:5] = FloatTensor(checkerboard)\n",
    "\n",
    "occ = OcclusionTransform(width=11)\n",
    "model_results, model = results['unsupervised_anomaly_data_validator'][METHOD]['resnet50_backbone_fashion_mnist_image_results']\n",
    "res_min, res_max = min(model_results.values()), max(model_results.values())\n",
    "interp_results = occlusion_interpretability(img, model, occ, (res_min, res_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[7]\n",
    "img = sample['fashion_mnist_image']\n",
    "\n",
    "img_altered = copy.deepcopy(img)\n",
    "checkerboard = np.indices((5, 5)).sum(axis=0) % 2\n",
    "img[:,11:16,11:16] = FloatTensor(checkerboard)\n",
    "\n",
    "occ = OcclusionTransform(width=11)\n",
    "model_results, model = results['unsupervised_anomaly_data_validator'][METHOD]['resnet50_backbone_fashion_mnist_image_results']\n",
    "res_min, res_max = min(model_results.values()), max(model_results.values())\n",
    "interp_results = occlusion_interpretability(img, model, occ, (res_min, res_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b281ef7",
   "metadata": {},
   "source": [
    "# Nonlocalized Anomaly Benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bdfeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(img.shape)* 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95d45b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = dataset[7]\n",
    "img = sample['fashion_mnist_image']\n",
    "img = copy.copy(img)\n",
    "img += noise\n",
    "# plt.imshow(img.reshape(img.shape[-2:]))\n",
    "\n",
    "occ = OcclusionTransform(width=5)\n",
    "occed = occ(img, (15, 15))\n",
    "\n",
    "model_results, model = results['unsupervised_anomaly_data_validator'][METHOD]['resnet50_backbone_fashion_mnist_image_results']\n",
    "res_min, res_max = min(model_results.values()), max(model_results.values())\n",
    "occlusion_interpretability(img, model, occ, (res_min, res_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6605b07",
   "metadata": {},
   "source": [
    "# Occlusion Transparency Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d44162",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_results\n",
    "original_image = dataset[529]['fashion_mnist_image']\n",
    "original_image.reshape((28, 28))\n",
    "interp_results.shape\n",
    "\n",
    "img = original_image\n",
    "heatmap = interp_results\n",
    "\n",
    "colormap = 'plasma'\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "ax2 = plt.subplot(1, 4, 3, aspect='equal')\n",
    "ax2.imshow(np.squeeze(img), alpha = .75, cmap='Greys_r')\n",
    "hm = ax2.imshow(heatmap, alpha = 0.25, cmap='plasma')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
