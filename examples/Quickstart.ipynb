{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961c370f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d45ce",
   "metadata": {},
   "source": [
    "In this quickstart, we will get Data Detective running on your dataset as quickly as possible.\n",
    "\n",
    "To get up and running on your own dataset as quickly as possible, we have formatted the tutorial so that sections requiring your own code / inputs are **bolded**; all other sections can be customized as needed, but can be skimmed / run as-is. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1139eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from typing import Dict, Union\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.pardir, '.'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from constants import FloatTensor\n",
    "from src.data_detective_engine import DataDetectiveEngine\n",
    "from src.enums.enums import DataType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87d144",
   "metadata": {},
   "source": [
    "## **Step 1: Dataset Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ae82e",
   "metadata": {},
   "source": [
    "### **Option 1: CSV Dataset Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994ecf0",
   "metadata": {},
   "source": [
    "The easiest way to get started with Data Detective for CSV data is to use the `CSVDataset` class. This class accepts the path for a CSV file as well as a dictionary containing the datatypes for each column in the CSV file. \n",
    "\n",
    "The CSV file can contain numbers, text, or images represented in the CSV file as absolute paths. The datatype options available in the CSV Dict include: \n",
    "- `DataType.CONTINUOUS`\n",
    "- `DataType.MULTIDIMENSIONAL` \n",
    "- `DataType.CATEGORICAL` \n",
    "- `DataType.TEXT`\n",
    "- `DataType.IMAGE`\n",
    "- `DataType.SEQUENTIAL`\n",
    "\n",
    "If it suits your use case, fill in the blank code is available below to create the CSVDataset below. Otherwise, skip to `Dataset Construction` to find out how to build your own dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.csv_dataset import CSVDataset\n",
    "\n",
    "dataset = CSVDataset(\n",
    "    # change filepath to your csv filepath\n",
    "    filepath=\"your_csv_filepath.csv\",\n",
    "    # change dictionary to map from csv filenames to data types\n",
    "    datatypes={\n",
    "        \"column1\": DataType.CONTINUOUS,\n",
    "        \"column2\": DataType.MULTIDIMENSIONAL,\n",
    "        # ...\n",
    "        \"column_k\": DataType.IMAGE,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9acc3",
   "metadata": {},
   "source": [
    "Note: if there is an `IMAGE` column in the CSV dataset that contains image paths, they will automatically be loaded into the dataset via `np.load`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a458d",
   "metadata": {},
   "source": [
    "### **Option 2: Dataset Construction**\n",
    "\n",
    "If dealing with data that does not easily serialize in CSV format, it is easier to create your own dataset to work within the Data Detective framework. Your dataset needs to satisfy the following requirements: \n",
    "\n",
    "1. It must override the `__getitem__` method that returns a dictionary mapping from each data column key to the data value. \n",
    "2. It must contain a `datatypes` method that returns a dictionary mapping from each data column key to the column's datatype. \n",
    "3. It must inherit from `torch.utils.data.DataType`.\n",
    "4. \\[optional\\] It is convenient, but not necessary, to define a `__len__` method. `\n",
    "\n",
    "\n",
    "Before diving in, let's look at a very simple dataset that consists of 10 columns of normal random variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from src.datasets.data_detective_dataset import DataDetectiveDataset\n",
    "\n",
    "class NormalDataset(DataDetectiveDataset):\n",
    "    def __init__(self, num_cols: int = 10, dataset_size: int = 1000, loc: float = 0.):\n",
    "        \"\"\"\n",
    "        Creates a normal dataset with column `feature_k` for k in [0, num_cols) \n",
    "        @param num_cols: number of columns to have\n",
    "        @param dataset_size: number of datapoints to have\n",
    "        @param loc: the mean of the data. \n",
    "        \"\"\"\n",
    "        self.dataset_size = dataset_size\n",
    "        self.columns = [f\"feature_{j}\" for j in range(num_cols)]\n",
    "\n",
    "        dataframe: DataFrame = pd.DataFrame({\n",
    "            f\"feature_{i}\": np.random.normal(loc, 100, size=dataset_size)\n",
    "            for i in range(num_cols)\n",
    "        }, columns=self.columns)\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        super().__init__(\n",
    "            show_id=False, \n",
    "            include_subject_id_in_data=False,\n",
    "            sample_ids = [str(s) for s in list(range(dataset_size))],\n",
    "            subject_ids = [str(s) for s in list(range(dataset_size))]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Returns a dict containing each column mapped to its value. \n",
    "        \"\"\"\n",
    "        return self.dataframe.iloc[index].to_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def datatypes(self) -> Dict[str, DataType]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary mapping each column to its datatype.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            column_name: DataType.CONTINUOUS\n",
    "            for column_name in self.columns\n",
    "        }\n",
    "\n",
    "\n",
    "class BrainStudy(torch.utils.data.Dataset):\n",
    "    def __init__(self, pathlist=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.df = pd.read_csv(pathlist) \n",
    "\n",
    "    def datatypes(self) -> Dict[str, DataType]:\n",
    "        \"\"\"\n",
    "        Specify datatype for each column name. \n",
    "            # SEQUENCE\n",
    "            # TEXT\n",
    "            # IMAGE\n",
    "            # CONTINUOUS\n",
    "            # CATEGORICAL\n",
    "            # MULTIDIMENSIONAL  vs  MULTIVARIATE vs VECTOR\n",
    "        \n",
    "            DataType.CONTINUOUS\n",
    "            DataType.CATEGORICAL\n",
    "            DataType.MULTIDIMENSIONAL  # maybe replace with MULTIVARIATE\n",
    "            DataType.IMAGE  # maybe replace with GRID\n",
    "            DataType.TIME_SERIES  # maybe replace with SEQUENCE\n",
    "\n",
    "            okay what needs to happen to make this semi-working?\n",
    "                - age: depictable by normal distribution or something.\n",
    "                - sex: 50/50\n",
    "                - cognitive score: uniform random normal\n",
    "                - clin history?\n",
    "        \"\"\"\n",
    "        self.datatype_dict =  {\n",
    "            \"age\": DataType.CONTINUOUS,\n",
    "            \"sex\": DataType.CATEGORICAL,\n",
    "            \"cognitive_score\": DataType.CONTINUOUS,\n",
    "            \"clinical_history\": DataType.TEXT,#path to .txt?\n",
    "            \"brain_MRI\": DataType.IMAGE, #path\n",
    "            \"brain_PET\": DataType.IMAGE,#path\n",
    "            \"activity_monitor\": DataType.SEQUENCE,#path\n",
    "            \"speech_derived_features\": DataType.MULTIVARIATE,#path to .npy ?\n",
    "        }\n",
    "        return self.datatype_dict    \n",
    "        \n",
    "    def __getitem__(self, idx: Union[int, slice, list]) -> Dict[str, Union[FloatTensor, int]]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary with column names and values for a specific idx. \n",
    "        \"\"\"\n",
    "        # for data inputs that are paths, code to load the file from the path needs to be provided in getitem\n",
    "        # code to load in sequence, image, or multivariate data\n",
    "        return {k:self.df[k][idx] for k in self.datatype_dict.keys()}\n",
    "\n",
    "\n",
    "# data_object = BrainStudy(pathlist='/path/to/my/pathlist.csv')\n",
    "\n",
    "# what it would look like for several splits\n",
    "# data_object = {'train': BrainStudy(pathlist='/path/to/my/train.csv'),\n",
    "#                'val':BrainStudy(pathlist='/path/to/my/val.csv'),\n",
    "#                'internal_test':BrainStudy(pathlist='/path/to/my/internal_test.csv'),}\n",
    "\n",
    "\n",
    "data_detective_engine = DataDetectiveEngine()\n",
    "\n",
    "dataset = NormalDataset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9441525",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd5898",
   "metadata": {},
   "source": [
    "Above, you can see that the dataset has both of the requirements above:\n",
    "\n",
    "1. It overrides `__getitem__` to provide a dict mapping from each column to a single value. \n",
    "2. It overrides `datatypes` to map the same keys in `__getitem__` to their datatypes. \n",
    "3. It inherits from `torch.utils.data.Dataset`.\n",
    "\n",
    "For complete clarity, let's take a look at the outputs of (1) and (2) below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e10d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44839292",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.datatypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169a354",
   "metadata": {},
   "source": [
    "Note that both dictionaries contain identical keys, indicating that no datatypes are missed in the definition of the `datatypes` function. \n",
    "\n",
    "Below is the skeleton code for a dataset construction. Fill it in with your desired implemenetation of `__getitem__` and `datatypes`, and any initialization you may need to do.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Sets up the dataset. This can include steps like:\n",
    "            - loading csv paths\n",
    "            - reading in text data\n",
    "            - cleaning and preprocessing\n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        YOUR CODE HERE\n",
    "        PUT YE CODE HERE, MATEY\n",
    "        ARR\n",
    "        \"\"\"\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Returns a dict containing each column mapped to its value. \n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        YOUR CODE HERE\n",
    "        AHOY, YE SCURVY CODER! WRITE YER MAGIC HERE!\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        return self.dataframe.iloc[index].to_dict()\n",
    "\n",
    "    def datatypes(self) -> Dict[str, DataType]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary mapping each column to its datatype.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        YOUR CODE HERE\n",
    "        AHOY, YE SCURVY CODER! WRITE YER MAGIC HERE!\n",
    "        \"\"\"\n",
    "\n",
    "    # NOTE: convenient, but not optional, to add __len__ method\n",
    "    # def __len__(self) -> int: \n",
    "    #     pass\n",
    "\n",
    "# put initialization code here or fix if needed\n",
    "dataset = YourDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a9e06",
   "metadata": {},
   "source": [
    "Now that you've written your dataset, lets make sure everything is in ship shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb357335",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.datatypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bef97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(isinstance(dataset[0], dict))\n",
    "assert(isinstance(dataset.datatypes(), dict))\n",
    "assert(dataset[0].keys() == dataset.datatypes().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a066a7",
   "metadata": {},
   "source": [
    "# Step 2: Data Object Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87def2d",
   "metadata": {},
   "source": [
    "The *data object* is a dictionary that consists of the preprocessed dataset and (optionally) its splits. More information about setting up the data object is available in the [main tutorial](Tutorial.ipynb) and the [ExtendingDD tutorial](ExtendingDD.ipynb).; for the purpose of the quickstart, splitting and organization is done for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.data_detective_dataset import dd_random_split\n",
    "\n",
    "\n",
    "inference_size: int = 20\n",
    "everything_but_inference_size: int = dataset.__len__() - inference_size\n",
    "inference_dataset, everything_but_inference_dataset = dd_random_split(dataset, [inference_size, dataset.__len__() - inference_size])\n",
    "    \n",
    "train_size: int = int(0.6 * len(everything_but_inference_dataset))\n",
    "val_size: int = int(0.2 * len(everything_but_inference_dataset))\n",
    "test_size: int = len(everything_but_inference_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = dd_random_split(everything_but_inference_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "data_object = {\n",
    "    \"entire_set\": dataset,\n",
    "    \"everything_but_inference_set\": everything_but_inference_dataset,\n",
    "    \"inference_set\": inference_dataset,\n",
    "    # unordered splits belong here\n",
    "    # in this example, train/val/test are included, but this section can be as long\n",
    "    # as desired and can contain an arbitrary number of named splits \n",
    "    \"train/val/test\": {\n",
    "        \"training_set\": train_dataset,\n",
    "        \"validation_set\": val_dataset,\n",
    "        \"test_set\": test_dataset,\n",
    "    },\n",
    "    # Example of k-fold split:\n",
    "    # \"fold_0\": {\n",
    "    #      \"training_set\": train_datasets[0],\n",
    "    #      \"test_set\": test_datasets[0],\n",
    "    # },\n",
    "    # \"fold_1\": {\n",
    "    #      \"training_set\": train_datasets[1],\n",
    "    #      \"test_set\": test_datasets[1],\n",
    "    # },\n",
    "    # ...\n",
    "    # \"fold_k\": {\n",
    "    #      \"training_set\": train_datasets[j],\n",
    "    #      \"test_set\": test_datasets[j],\n",
    "    # }\n",
    "}\n",
    "\n",
    "print(f\"size of inference_dataset: {inference_dataset.__len__()}\")\n",
    "print(f\"size of everything_but_inference_dataset: {everything_but_inference_dataset.__len__()}\")\n",
    "print(f\"size of train_dataset: {train_dataset.__len__()}\")\n",
    "print(f\"size of entire dataset: {dataset.__len__()}\")\n",
    "print(f\"size of val_dataset: {val_dataset.__len__()}\")\n",
    "print(f\"size of test_dataset: {test_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2197dc",
   "metadata": {},
   "source": [
    "# Step 3: Setting up a Validation Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be22c1",
   "metadata": {},
   "source": [
    "## Step 3.1: Specifying Validators and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033c9e0",
   "metadata": {},
   "source": [
    "The validation schema contains information about the types of checks that will be executed by the Data Detective Engine and the transforms that Data Detective will use. More detailsd about creating your own validation schema is available in the [main tutorial](Tutorial.ipynb); below is the validation schema that we recommend to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_schema : Dict = {\n",
    "    \"validators\": {\n",
    "        \"unsupervised_anomaly_data_validator\": {},\n",
    "        \"unsupervised_multimodal_anomaly_data_validator\": {},\n",
    "        # \"split_covariate_data_validator\": {},\n",
    "        # \"ood_inference_data_validator\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f89af",
   "metadata": {},
   "source": [
    "## Step 3.2: Specifying Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1525d",
   "metadata": {},
   "source": [
    "It may be the case that you are using a data modality that has little to no method infrastructure in Data Detective. The simplest way to make use of all of Data Detective's functionality is to use a transform that maps this data modality to a well-supported modality in Data Detective such as multidimensional data. In our example, we will be making use of a pretrained resnet50 backbone to map images to 2048 dimensional vectors. This will allow us to make use of methods built for multidimensional data on our image representations. \n",
    "\n",
    "More information about introducing custom transforms into Data Detective and customizing the transform schema with pre-existing transforms is available in the [main tutorial](Tutorial.ipynb) and explanations on how to create/use your own transforms are available in the [ExtendingDD tutorial](ExtendingDD.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_schema : Dict = {\n",
    "    \"transforms\": {\n",
    "        \"IMAGE\": [{\n",
    "            \"name\": \"resnet50\",\n",
    "            \"in_place\": \"False\",\n",
    "            \"options\": {},\n",
    "        }],\n",
    "    }\n",
    "}\n",
    "     \n",
    "full_validation_schema: Dict = {\n",
    "    **validation_schema, \n",
    "    **transform_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3611d3",
   "metadata": {},
   "source": [
    "# Step 4: Running the Data Detective Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecb162",
   "metadata": {},
   "source": [
    "Now that the full validation schema and data object are prepared, we are ready to run the Data Detective Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ee2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_detective_engine = DataDetectiveEngine()\n",
    "\n",
    "start_time = time.time()\n",
    "results = data_detective_engine.validate_from_schema(full_validation_schema, data_object)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e1d58",
   "metadata": {},
   "source": [
    "Great! Let's start to look at and analyze the results we've collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84578eae",
   "metadata": {},
   "source": [
    "# Step 5: Interpreting Results using the Built-In Rank Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf43ee8",
   "metadata": {},
   "source": [
    "To do rank aggregation, create a rankings object and either aggregate completely with the `aggregate_results_modally` or aggregate by a single modality with the `aggregate_results_multimodally`. See below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import List\n",
    "\n",
    "from src.aggregation.rankings import RankingAggregationMethod, ResultAggregator\n",
    "\n",
    "aggregator = ResultAggregator(results_object=results)\n",
    "# modal_rankings = aggregator.aggregate_results_modally(\"unsupervised_anomaly_data_validator\", [RankingAggregationMethod.LOWEST_RANK, RankingAggregationMethod.HIGHEST_RANK, RankingAggregationMethod.ROUND_ROBIN], given_data_modality=\"feature_0\")\n",
    "total_rankings = aggregator.aggregate_results_multimodally(\"unsupervised_multimodal_anomaly_data_validator\", [RankingAggregationMethod.LOWEST_RANK, RankingAggregationMethod.HIGHEST_RANK])\n",
    "total_rankings.round(2).sort_values(\"lowest_rank_agg_rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db73b16",
   "metadata": {},
   "source": [
    "\n",
    "### Appendix 1A: Complete list of validator methods\n",
    "\n",
    "| name | path | method description | data types | operable split types | \n",
    "| ---- | ---- | ------------------ | ---------- | -------------------- | \n",
    "| adbench_validator_method | src/validator_methods/validator_method_factories/adbench_validator_method_factory.py | factory generating adbench methods that perform anomaly detection | multidimensional data | entire set | \n",
    "| adbench_multimodal_validator_method | src/validator_methods/validator_method_factories/adbench_multimodal_validator_method_factory.py | factory generating adbench methods that perform anomaly detection by concatenating all multidimensional columns first to be able to draw conclusions jointly from the data | multidimensional data | entire set | \n",
    "| adbench_ood_inference_validator_method | src/validator_methods/validator_method_factories/adbench_ood_inference_validator_method_factory.py | factory generating methods that perform ood testing given a source set and a target/inference set using adbench emthods | multidimensional data | inference_set, everything_but_inference_set | \n",
    "| chi square validator method | src/validator_methods/chi_square_validator_method.py | chi square test for testing CI assumptions between two categorical variables | categorical data | entire_set |\n",
    "| diffi anomaly explanation validator method | src/validator_methods/diffi_anomaly_explanation_validator_method.py | A validator method for explainable anomaly detection using the DIFFI feature importance method. | multidimensional | entire_set |\n",
    "| fcit validator method | src/validator_methods/fcit_validator_method.py | A method for determining conditionanl independence of two multidimensional vectors given a third. | continuous, categorical, or multidimensional | entire_set |\n",
    "| kolmogorov smirnov multidimensional split validator | src/validator_methods/kolmogorov_smirnov_multidimensional_split_validator_method.py | KS testing over multidimensional data for split covariate shift. | multidimensional | entire_set |\n",
    "| kolmogoriv smirnov normality validator method | src/validator_methods/kolmogorov_smirnov_normality_validator_method.py | KS testing over continuous data for normality assumption. | continuous | entire_set | \n",
    "| kolmogorov smirnov split validator method | src/validator_methods/kolmogorov_smirnov_split_validator_method.py | KS testing over continuous data for split covariate shift. |  continuous | entire_set |  \n",
    "| kruskal wallis multidimensional split validator method | src/validator_methods/kruskal_wallis_multidimensional_split_validator_method.py | kruskal wallis testing over multidimensional data for split covariate shift. | multidimensional | entire_set | \n",
    "| kruskal wallis split validator method | src/validator_methods/kruskal_wallis_split_validator_method.py | kruskal wallis testing over continuous data for split covariate shift. | continuous | entire_set |  \n",
    "| mann whitney multidimensional split validator method | src/validator_methods/mann_whitney_multidimensional_split_validator_method.py | mann whitney testing over multidimensional data for split covariate shift. | multidimensional | entire_set |\n",
    "| mann whitney split validator method | src/validator_methods/mann_whitney_split_validator_method.py | mann whitney testing over continuous data for split covariate shift. | continuous | entire_set |  \n",
    "| duplicate high dimensional validator method | src/validator_methods/duplicate_high_dimensional_validator_method.py | multidimensional, text, sequence | entire_set |\n",
    "| duplicate sample validator method |src/validator_methods/duplicate_sample_validator_method.py  | any | entire_set |\n",
    "| near duplicate multidimensional validator method | src/validator_methods/near_duplicate_multidimensional_validator_method.py | multidimensional | entire_set |\n",
    "| near duplicate sample validator method | src/validator_methods/near_duplicate_sample_validator_method.py | multidimensional, continuous, categorical | entire_set | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
