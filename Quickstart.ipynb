{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961c370f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d45ce",
   "metadata": {},
   "source": [
    "In this quickstart, we will get Data Detective running on your dataset as quickly as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1139eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from typing import Dict, Union\n",
    "\n",
    "from constants import FloatTensor\n",
    "from src.aggregation.rankings import RankingAggregator, RankingAggregationMethod\n",
    "from src.data_detective_engine import DataDetectiveEngine\n",
    "from src.datasets.tutorial_dataset import TutorialDataset\n",
    "from src.enums.enums import DataType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87d144",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ae82e",
   "metadata": {},
   "source": [
    "### CSV Dataset Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994ecf0",
   "metadata": {},
   "source": [
    "The easiest way to get started with Data Detective for CSV data is to use the `CSVDataset` class. This class accepts the path for a CSV file as well as a dictionary containing the datatypes for each column in the CSV file. \n",
    "\n",
    "The CSV file can contain numbers, text, or images represented in the CSV file as absolute paths. The datatype options available in the CSV Dict include: \n",
    "- `DataType.CONTINUOUS`\n",
    "- `DataType.MULTIDIMENSIONAL` \n",
    "- `DataType.CATEGORICAL` \n",
    "- `DataType.TEXT`\n",
    "- `DataType.IMAGE`\n",
    "- `DataType.SEQUENTIAL`\n",
    "\n",
    "If it suits your use case, fill in the blank code is available below to create the CSVDataset below. Otherwise, skip to `Dataset Construction` to find out how to build your own dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.csv_dataset import CSVDataset\n",
    "\n",
    "dataset = CSVDataset(\n",
    "    # change filepath to your csv filepath\n",
    "    filepath=\"your_csv_filepath.csv\",\n",
    "    # change dictionary to map from csv filenames to data types\n",
    "    datatypes={\n",
    "        \"column1\": DataType.CONTINUOUS,\n",
    "        \"column2\": DataType.MULTIDIMENSIONAL,\n",
    "        # ...\n",
    "        \"column_k\": DataType.IMAGE,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9acc3",
   "metadata": {},
   "source": [
    "Note: if there is an `IMAGE` column in the CSV dataset that contains image paths, they will automatically be loaded into the dataset via `np.load`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a458d",
   "metadata": {},
   "source": [
    "### Dataset Construction\n",
    "\n",
    "If dealing with data that does not easily serialize in CSV format, it is easier to create your own dataset to work within the Data Detective framework. Your dataset needs to satisfy the following requirements: \n",
    "\n",
    "1. It must override the `__getitem__` method that returns a dictionary mapping from each data column key to the data value. \n",
    "2. It must contain a `datatypes` method that returns a dictionary mapping from each data column key to the column's datatype. \n",
    "3. It must inherit from `torch.utils.data.DataType`.\n",
    "4. \\[optional\\] It is convenient, but not necessary, to define a `__len__` method. `\n",
    "\n",
    "\n",
    "Before diving in, let's look at a very simple dataset that consists of 10 columns of normal random variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4fb079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NormalDataset(Dataset):\n",
    "    def __init__(self, num_cols: int = 10, dataset_size: int = 10000, loc: float = 0.):\n",
    "        \"\"\"\n",
    "        Creates a normal dataset with column `feature_k` for k in [0, num_cols) \n",
    "        @param num_cols: number of columns to have\n",
    "        @param dataset_size: number of datapoints to have\n",
    "        @param loc: the mean of the data. \n",
    "        \"\"\"\n",
    "        self.dataset_size = dataset_size\n",
    "        self.columns = [f\"feature_{j}\" for j in range(num_cols)]\n",
    "\n",
    "        dataframe: DataFrame = pd.DataFrame({\n",
    "            f\"feature_{i}\": np.random.normal(loc, 1, size=dataset_size)\n",
    "            for i in range(num_cols)\n",
    "        }, columns=self.columns)\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Returns a dict containing each column mapped to its value. \n",
    "        \"\"\"\n",
    "        return self.dataframe.iloc[index].to_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def datatypes(self) -> Dict[str, DataType]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary mapping each column to its datatype.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            column_name: DataType.CONTINUOUS\n",
    "            for column_name in self.columns\n",
    "        }\n",
    "\n",
    "dataset = NormalDataset() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd5898",
   "metadata": {},
   "source": [
    "Above, you can see that the dataset has both of the requirements above:\n",
    "\n",
    "1. It overrides `__getitem__` to provide a dict mapping from each column to a single value. \n",
    "2. It overrides `datatypes` to map the same keys in `__getitem__` to their datatypes. \n",
    "3. It inherits from `torch.utils.data.Dataset`.\n",
    "\n",
    "For complete clarity, let's take a look at the outputs of (1) and (2) below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e10d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_0': -1.8818520390267668,\n",
       " 'feature_1': 0.48079518810297267,\n",
       " 'feature_2': 0.3253609247058997,\n",
       " 'feature_3': -0.8423106628622142,\n",
       " 'feature_4': -0.8097134235785098,\n",
       " 'feature_5': -2.2650311419827744,\n",
       " 'feature_6': -0.5544628629028009,\n",
       " 'feature_7': 0.06647392399256227,\n",
       " 'feature_8': 0.8387809291885946,\n",
       " 'feature_9': -0.34279014635692356}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44839292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_0': <DataType.CONTINUOUS: 'continuous'>,\n",
       " 'feature_1': <DataType.CONTINUOUS: 'continuous'>,\n",
       " 'feature_2': <DataType.CONTINUOUS: 'continuous'>,\n",
       " 'feature_3': <DataType.CONTINUOUS: 'continuous'>,\n",
       " 'feature_4': <DataType.CONTINUOUS: 'continuous'>,\n",
       " 'feature_5': <DataType.CONTINUOUS: 'continuous'>,\n",
       " 'feature_6': <DataType.CONTINUOUS: 'continuous'>,\n",
       " 'feature_7': <DataType.CONTINUOUS: 'continuous'>,\n",
       " 'feature_8': <DataType.CONTINUOUS: 'continuous'>,\n",
       " 'feature_9': <DataType.CONTINUOUS: 'continuous'>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.datatypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169a354",
   "metadata": {},
   "source": [
    "Note that both dictionaries contain identical keys, indicating that no datatypes are missed in the definition of the `datatypes` function. \n",
    "\n",
    "Below is the skeleton code for a dataset construction. Fill it in with your desired implemenetation of `__getitem__` and `datatypes`, and any initialization you may need to do.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Sets up the dataset. This can include steps like:\n",
    "            - loading csv paths\n",
    "            - reading in text data\n",
    "            - cleaning and preprocessing\n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        YOUR CODE HERE\n",
    "        PUT YE CODE HERE, MATEY\n",
    "        ARR\n",
    "        \"\"\"\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Returns a dict containing each column mapped to its value. \n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        YOUR CODE HERE\n",
    "        AHOY, YE SCURVY CODER! WRITE YER MAGIC HERE!\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        return self.dataframe.iloc[index].to_dict()\n",
    "\n",
    "    def datatypes(self) -> Dict[str, DataType]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary mapping each column to its datatype.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        YOUR CODE HERE\n",
    "        AHOY, YE SCURVY CODER! WRITE YER MAGIC HERE!\n",
    "        \"\"\"\n",
    "\n",
    "    # NOTE: conveninet, but not optional, to add __len__ method\n",
    "    # def __len__(self) -> int: \n",
    "    #     pass\n",
    "\n",
    "# put initialization code here or fix if needed\n",
    "dataset = YourDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a9e06",
   "metadata": {},
   "source": [
    "Now that you've written your dataset, lets make sure everything is in ship shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb357335",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.datatypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59bef97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(isinstance(dataset[0], dict))\n",
    "assert(isinstance(dataset.datatypes(), dict))\n",
    "assert(dataset[0].keys() == dataset.datatypes().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a066a7",
   "metadata": {},
   "source": [
    "# Step 2: Data Object Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87def2d",
   "metadata": {},
   "source": [
    "The *data object* is a dictionary that consists of the preprocessed dataset and (optionally) its splits. More information about setting up the data object is available in the tutorial; for the purpose of the quickstart, splitting and organization is done for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1870a4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of inference_dataset: 20\n",
      "size of everything_but_inference_dataset: 9980\n",
      "size of train_dataset: 5988\n",
      "size of entire dataset: 10000\n",
      "size of val_dataset: 1996\n",
      "size of test_dataset: 1996\n"
     ]
    }
   ],
   "source": [
    "inference_size: int = 20\n",
    "everything_but_inference_size: int = dataset.__len__() - inference_size\n",
    "inference_dataset, everything_but_inference_dataset = torch.utils.data.random_split(dataset, [inference_size, dataset.__len__() - inference_size])\n",
    "    \n",
    "train_size: int = int(0.6 * len(everything_but_inference_dataset))\n",
    "val_size: int = int(0.2 * len(everything_but_inference_dataset))\n",
    "test_size: int = len(everything_but_inference_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(everything_but_inference_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "data_object = {\n",
    "    \"entire_set\": dataset,\n",
    "    \"everything_but_inference_set\": everything_but_inference_dataset,\n",
    "    \"inference_set\": inference_dataset,\n",
    "    \"split_group_set\": {\n",
    "          # unordered splits belong here\n",
    "          # in this example, train/val/test are included, but this dict can be as long\n",
    "          # as desired and can contain an arbitrary number of named splits \n",
    "          \"train/val/test\": {\n",
    "               \"training_set\": train_dataset,\n",
    "               \"validation_set\": val_dataset,\n",
    "               \"test_set\": test_dataset,\n",
    "          },\n",
    "        # Example of k-fold split:\n",
    "        # \"fold_0\": {\n",
    "        #      \"training_set\": train_datasets[0],\n",
    "        #      \"test_set\": test_datasets[0],\n",
    "        # },\n",
    "        # \"fold_1\": {\n",
    "        #      \"training_set\": train_datasets[1],\n",
    "        #      \"test_set\": test_datasets[1],\n",
    "        # },\n",
    "        # ...\n",
    "        # \"fold_k\": {\n",
    "        #      \"training_set\": train_datasets[j],\n",
    "        #      \"test_set\": test_datasets[j],\n",
    "        # }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"size of inference_dataset: {inference_dataset.__len__()}\")\n",
    "print(f\"size of everything_but_inference_dataset: {everything_but_inference_dataset.__len__()}\")\n",
    "print(f\"size of train_dataset: {train_dataset.__len__()}\")\n",
    "print(f\"size of entire dataset: {dataset.__len__()}\")\n",
    "print(f\"size of val_dataset: {val_dataset.__len__()}\")\n",
    "print(f\"size of test_dataset: {test_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2197dc",
   "metadata": {},
   "source": [
    "# Step 3: Setting up a Validation Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be22c1",
   "metadata": {},
   "source": [
    "## Step 3.1: Specifying Validators and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033c9e0",
   "metadata": {},
   "source": [
    "The validation schema contains information about the types of checks that will be executed by the Data Detective Engine and the transforms that Data Detective will use. More detailsd about creating your own validation schema is available in the tutorial; below is the validation schema that we recommend to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbdc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_schema : Dict = {\n",
    "    \"default_inclusion\": False,\n",
    "    \"validators\": {\n",
    "        \"unsupervised_anomaly_data_validator\": {},\n",
    "        \"unsupervised_multimodal_anomaly_data_validator\": {},\n",
    "        \"split_covariate_data_validator\": {},\n",
    "        \"ood_inference_data_validator\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f89af",
   "metadata": {},
   "source": [
    "## Step 3.2: Specifying Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1525d",
   "metadata": {},
   "source": [
    "It may be the case that you are using a data modality that has little to no method infrastructure in Data Detective. The simplest way to make use of all of Data Detective's functionality is to use a transform that maps this data modality to a well-supported modality in Data Detective such as multidimensional data. In our example, we will be making use of a pretrained resnet50 backbone to map images to 2048 dimensional vectors. This will allow us to make use of methods built for multidimensional data on our image representations. \n",
    "\n",
    "More information about introducing custom transforms into Data Detective and cusotmizing the transform schema is available in the main tutorial and the ExtendingDD tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_schema : Dict = {\n",
    "    \"transforms\": {\n",
    "        \"image\": [{\n",
    "            \"name\": \"resnet50\",\n",
    "            \"in_place\": \"False\",\n",
    "            \"options\": {},\n",
    "        }],\n",
    "    }\n",
    "}\n",
    "     \n",
    "full_validation_schema: Dict = {\n",
    "    **validation_schema, \n",
    "    **transform_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3611d3",
   "metadata": {},
   "source": [
    "# Step 4: Running the Data Detective Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecb162",
   "metadata": {},
   "source": [
    "Now that the full validation schema and data object are prepared, we are ready to run the Data Detective Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692ee2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running validator class unsupervised_anomaly_data_validator...\n",
      "running validator class unsupervised_multimodal_anomaly_data_validator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window(order, start, length):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/links.py:5: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def identity(x):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/links.py:10: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _identity_inverse(x):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/links.py:15: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def logit(x):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/links.py:20: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _logit_inverse(x):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running validator class split_covariate_data_validator...\n",
      "running validator class ood_inference_data_validator...\n",
      "thread 11393855488 entered to handle validator method iforest_anomaly_validator_methodthread 11410681856 entered to handle validator method cblof_anomaly_validator_method\n",
      "thread 11410681856:    running cblof_anomaly_validator_method...\n",
      "\n",
      "thread 11393855488:    running iforest_anomaly_validator_method...\n",
      "thread 11427508224 entered to handle validator method pca_anomaly_validator_method\n",
      "thread 11427508224:    running pca_anomaly_validator_method...\n",
      "thread 11444334592 entered to handle validator method iforest_multimodal_anomaly_validator_method\n",
      "thread 11444334592:    running iforest_multimodal_anomaly_validator_method...\n",
      "thread 11461160960 entered to handle validator method pca_multimodal_anomaly_validator_method\n",
      "thread 11461160960:    running pca_multimodal_anomaly_validator_method...\n",
      "thread 11477987328 entered to handle validator method cblof_multimodal_anomaly_validator_method\n",
      "thread 11477987328:    running cblof_multimodal_anomaly_validator_method...\n",
      "thread 11494813696 entered to handle validator method mann_whitney_multidimensional_split_validator_method\n",
      "thread 11494813696:    running mann_whitney_multidimensional_split_validator_method...\n",
      "thread 11511640064 entered to handle validator method mann_whitney_split_validator_method\n",
      "thread 11511640064:    running mann_whitney_split_validator_method...\n",
      "thread 11528466432 entered to handle validator method kolmogorov_smirnov_split_validator_method\n",
      "thread 11528466432:    running kolmogorov_smirnov_split_validator_method...\n",
      "thread 11545292800 entered to handle validator method kruskal_wallis_multidimensional_split_validator_method\n",
      "thread 11545292800:    running kruskal_wallis_multidimensional_split_validator_method...\n",
      "thread 11562119168 entered to handle validator method kolmogorov_smirnov_multidimensional_split_validator_method\n",
      "thread 11562119168:    running kolmogorov_smirnov_multidimensional_split_validator_method...\n",
      "thread 11578945536 entered to handle validator method kruskal_wallis_split_validator_method\n",
      "thread 11578945536:    running kruskal_wallis_split_validator_method...\n",
      "thread 11595771904 entered to handle validator method pca_anomaly_validator_method\n",
      "thread 11595771904:    running pca_anomaly_validator_method...\n",
      "thread 11612598272 entered to handle validator method cblof_anomaly_validator_method\n",
      "thread 11612598272:    running cblof_anomaly_validator_method...\n",
      "thread 11629424640 entered to handle validator method iforest_anomaly_validator_method\n",
      "thread 11629424640:    running iforest_anomaly_validator_method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 11427508224: finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 11461160960: finished\n",
      "thread 11511640064: finished\n",
      "thread 11528466432: finished\n",
      "thread 11578945536: finished\n",
      "thread 11595771904: finished\n",
      "thread 11444334592: finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 11629424640: finished\n",
      "thread 11393855488: finished\n",
      "thread 11612598272: finished\n",
      "thread 11477987328: finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 11562119168: finished\n",
      "thread 11410681856: finished\n",
      "thread 11494813696: finished\n",
      "thread 11545292800: finished\n",
      "--- 15.769446849822998 seconds ---\n"
     ]
    }
   ],
   "source": [
    "data_detective_engine = DataDetectiveEngine()\n",
    "\n",
    "# 1 thread, --- 220.85648322105408 seconds ---\n",
    "# multithreadinng (joblib), --- 149.11400604248047 seconds ---\n",
    "# thread pools, --- 81.38025784492493 seconds ---\n",
    "# data-level caching, clean cache, --- 75.22503590583801 seconds ---\n",
    "# sample-level caching, clean cache--- 26.184876918792725 seconds ---\n",
    "# data-level caching, dirty cache, --- 22.925609827041626 seconds ---\n",
    "# sample-level caching, dirty cache, --- 19.73765206336975 seconds ---\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "results = data_detective_engine.validate_from_schema(full_validation_schema, data_object)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e1d58",
   "metadata": {},
   "source": [
    "Great! Let's start to look at and analyze the results we've collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84578eae",
   "metadata": {},
   "source": [
    "# Step 5: Interpreting Results using the Built-In Rank Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf43ee8",
   "metadata": {},
   "source": [
    "To do rank aggregation, create a rankings object and either aggregate completely with the `aggregate_rankings` or aggregate by a single modality with the `aggregate_modal_rankings`. See below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from typing import List\n",
    "\n",
    "from src.aggregation.rankings import RankingAggregationMethod, RankingAggregator\n",
    "\n",
    "aggregator = RankingAggregator(results_object=results)\n",
    "modal_rankings = aggregator.aggregate_modal_rankings(\"unsupervised_anomaly_data_validator\", [RankingAggregationMethod.LOWEST_RANK, RankingAggregationMethod.HIGHEST_RANK, RankingAggregationMethod.ROUND_ROBIN], given_data_modality=\"feature_0\")\n",
    "total_rankings = aggregator.aggregate_rankings(\"unsupervised_anomaly_data_validator\", [RankingAggregationMethod.LOWEST_RANK, RankingAggregationMethod.HIGHEST_RANK, RankingAggregationMethod.ROUND_ROBIN])\n",
    "total_rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db73b16",
   "metadata": {},
   "source": [
    "### Appendix 1A: Complete list of validator methods\n",
    "\n",
    "| name | path | method description | data types | operable split types | \n",
    "| ---- | ---- | ------------------ | ---------- | -------------------- | \n",
    "| adbench_validator_method | src/validator_methods/validator_method_factories/adbench_validator_method_factory.py | factory generating adbench methods that perform anomaly detection | multidimensional data | entire set | \n",
    "| adbench_multimodal_validator_method | src/validator_methods/validator_method_factories/adbench_multimodal_validator_method_factory.py | factory generating adbench methods that perform anomaly detection by concatenating all multidimensional columns first to be able to draw conclusions jointly from the data | multidimensional data | entire set | \n",
    "| adbench_ood_inference_validator_method | src/validator_methods/validator_method_factories/adbench_ood_inference_validator_method_factory.py | factory generating methods that perform ood testing given a source set and a target/inference set using adbench emthods | multidimensional data | inference_set, everything_but_inference_set | \n",
    "| chi square validator method | src/validator_methods/chi_square_validator_method.py | chi square test for testing CI assumptions between two categorical variables | categorical data | entire_set |\n",
    "| diffi anomaly explanation validator method | src/validator_methods/diffi_anomaly_explanation_validator_method.py | A validator method for explainable anomaly detection using the DIFFI feature importance method. | multidimensional | entire_set |\n",
    "| fcit validator method | src/validator_methods/fcit_validator_method.py | A method for determining conditionanl independence of two multidimensional vectors given a third. | continuous, categorical, or multidimensional | entire_set |\n",
    "| kolmogorov smirnov multidimensional split validator | src/validator_methods/kolmogorov_smirnov_multidimensional_split_validator_method.py | KS testing over multidimensional data for split covariate shift. | multidimensional | entire_set |\n",
    "| kolmogoriv smirnov normality validator method | src/validator_methods/kolmogorov_smirnov_normality_validator_method.py | KS testing over continuous data for normality assumption. | continuous | entire_set | \n",
    "| kolmogorov smirnov split validator method | src/validator_methods/kolmogorov_smirnov_split_validator_method.py | KS testing over continuous data for split covariate shift. |  continuous | entire_set |  \n",
    "| kruskal wallis multidimensional split validator method | src/validator_methods/kruskal_wallis_multidimensional_split_validator_method.py | kruskal wallis testing over multidimensional data for split covariate shift. | multidimensional | entire_set | \n",
    "| kruskal wallis split validator method | src/validator_methods/kruskal_wallis_split_validator_method.py | kruskal wallis testing over continuous data for split covariate shift. | continuous | entire_set |  \n",
    "| mann whitney multidimensional split validator method | src/validator_methods/mann_whitney_multidimensional_split_validator_method.py | mann whitney testing over multidimensional data for split covariate shift. | multidimensional | entire_set |\n",
    "| mann whitney split validator method | src/validator_methods/mann_whitney_split_validator_method.py | mann whitney testing over continuous data for split covariate shift. | continuous | entire_set |  \n",
    "| shap tree validator method | src/validator_methods/shap_tree_validator_method.py |     A validator method for explainable anomaly detection using Shapley values. | multidimensional | entire_set | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6848ef9",
   "metadata": {},
   "source": [
    "### Appendix 1B: Complete list of validators\n",
    "| name | path | method description | \n",
    "| ---- | ---- | ------------------ | \n",
    "| test | test | test | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda0976",
   "metadata": {},
   "source": [
    "### Appendix 1C: Complete list of transforms. \n",
    "\n",
    "| name | path | method description | \n",
    "| ---- | ---- | ------------------ | \n",
    "| test | test | test | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
