{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961c370f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d45ce",
   "metadata": {},
   "source": [
    "In this tutorial, we will explain the basics of using Data Detective to perform a Data Investigation. In this tutorial, we will go through the steps of setting up and running a basic Data Detective Investigation, including: \n",
    "1. Configuring a dataset that works with Data Detective. \n",
    "2. Setting up a schema that defines the Data Detective investigation. \n",
    "3. Executing a data detective investigation. \n",
    "4. Summarizing results using the built-in Rank Aggregator\n",
    "\n",
    "\n",
    "We will also include supplemental tutorials for some of Data Detective's more advanced features, including: \n",
    "- Extending the Data Detective investigation with custom validations\n",
    "- Extending the transform library to map custom datatypes to supported datatypes\n",
    "\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1139eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from typing import Dict, Union\n",
    "\n",
    "from constants import FloatTensor\n",
    "from src.aggregation.rankings import RankingAggregator, RankingAggregationMethod\n",
    "from src.data_detective_engine import DataDetectiveEngine\n",
    "from src.datasets.tutorial_dataset import TutorialDataset\n",
    "from src.enums.enums import DataType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfa30b",
   "metadata": {},
   "source": [
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028623e",
   "metadata": {},
   "source": [
    "## Requirements for a Dataset\n",
    "\n",
    "For a dataset to work within the Data Detective framework, it needs to satisfy the following requirements: \n",
    "\n",
    "1. It must override the `__getitem__` method that returns a dictionary mapping from each data column key to the data value. \n",
    "2. It must contain a `datatypes` method that returns a dictionary mapping from each data column key to the column's datatype. \n",
    "\n",
    "Let's examine what this looks like in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87d144",
   "metadata": {},
   "source": [
    "## Dataset Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd5898",
   "metadata": {},
   "source": [
    "In this tutorial, we will create a heterogeneous dataset that consists of the following items: \n",
    "\n",
    "- MNIST images\n",
    "- MNIST labels\n",
    "- 10-dimensional normal distribution (μ=0, σ=1)\n",
    "\n",
    "The full dataset can be found under src/datasets/tutorial_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f6b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TutorialDataset(\n",
    "    root='./data/MNIST',\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor() \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7faeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show_datapoint(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a420b",
   "metadata": {},
   "source": [
    "# Setting up the Data Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87def2d",
   "metadata": {},
   "source": [
    "The *data object* consists of the preprocessed dataset and (optionally) its splits. In order to make use of split validation techniques such as distribution shift between splits, the data splits must be included. \n",
    "\n",
    "For the purpose of thsi tutorial, we are going to hold out 20 samples that we are going to use to model \"real world\" samples that will be encountered at inference time. We will then perform a 60/20/20 split between train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1870a4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of inference_dataset: 20\n",
      "size of everything_but_inference_dataset: 30\n",
      "size of train_dataset: 18\n",
      "size of entire dataset: 50\n",
      "size of val_dataset: 6\n",
      "size of test_dataset: 6\n"
     ]
    }
   ],
   "source": [
    "inference_size: int = 20\n",
    "everything_but_inference_size: int = dataset.__len__() - inference_size\n",
    "inference_dataset, everything_but_inference_dataset = torch.utils.data.random_split(dataset, [inference_size, dataset.__len__() - inference_size])\n",
    "    \n",
    "train_size: int = int(0.6 * len(everything_but_inference_dataset))\n",
    "val_size: int = int(0.2 * len(everything_but_inference_dataset))\n",
    "test_size: int = len(everything_but_inference_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(everything_but_inference_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "data_object: Dict[str, torch.utils.data.Dataset] = {\n",
    "    \"training_set\": train_dataset,\n",
    "    \"validation_set\": val_dataset,\n",
    "    \"test_set\": test_dataset,\n",
    "    \"entire_set\": dataset,\n",
    "    \"everything_but_inference_set\": everything_but_inference_dataset,\n",
    "    \"inference_set\": inference_dataset\n",
    "}\n",
    "\n",
    "new_data_object = {\n",
    "    \"entire_set\": dataset,\n",
    "    \"everything_but_inference_set\": everything_but_inference_dataset,\n",
    "    \"inference_set\": inference_dataset,\n",
    "    \"unordered_split_groups\": {\n",
    "        \"train/val/test\": {\n",
    "            \"training_set\": train_dataset,\n",
    "            \"validation_set\": val_dataset,\n",
    "            \"test_set\": test_dataset,\n",
    "       },\n",
    "    }\n",
    "}\n",
    "\n",
    "#### how do we want this to be applied?\n",
    "# - what method types are there?\n",
    "#   - methods that only operate on the entire set (easy)\n",
    "#   - methods that operate on a group of unordered splits (train/val/test, perhaps)\n",
    "#   - methods that operate on a group of ordered splits (inference / non-inference)\n",
    "#     - these have to be ordered/named anyways, so we might as well just keep them as is, which makes this easy too\n",
    "# - so, what we need to do \n",
    "\n",
    "print(f\"size of inference_dataset: {inference_dataset.__len__()}\")\n",
    "print(f\"size of everything_but_inference_dataset: {everything_but_inference_dataset.__len__()}\")\n",
    "print(f\"size of train_dataset: {train_dataset.__len__()}\")\n",
    "print(f\"size of entire dataset: {dataset.__len__()}\")\n",
    "print(f\"size of val_dataset: {val_dataset.__len__()}\")\n",
    "print(f\"size of test_dataset: {test_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2197dc",
   "metadata": {},
   "source": [
    "# Setting up a Validation Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be22c1",
   "metadata": {},
   "source": [
    "## Specifying Validators and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033c9e0",
   "metadata": {},
   "source": [
    "The validation schema contains information about the types of checks that will be executed by the Data Detective Engine and the transforms that Data Detective will use. Before discussing the validation schema, it is important to define two key modular components that make up Data Detective investigations. Data Detective's functionality can be divided into a modular, implementation-heavy component referred to as a *validator method* and an easily toggleable component referred to as a *validator*.\n",
    "\n",
    "A *validator method* performs a specific type of test for a data issue on a specific data type. These validator methods primarily consist of the code needed to take a dataset and run an evaluation on it that produces either a positive or negative result or a score. Some examples of validator methods include:\n",
    "- Mann-Whitney U-Test to examine distribution shift between train/test splits on tabular data\n",
    "- Kernel Conditional Independence (KCI) test for validating causal assumptions on vectorvalued\n",
    "data\n",
    "- Isolation forest trained/evaluated on image histograms for identifying anomalies in imaging\n",
    "data\n",
    "\n",
    "*Validations* are toggleable, datatype-agnostic collections of validator methods that are serially applied to the dataset. Each validator targets a single problem that may arise in data, including: \n",
    "- Shift between different data splits\n",
    "- Outlier and anomaly detection\n",
    "- Violation of parametric assumptions on the data\n",
    "- Violation of expected casual structures / conditional independences in the \n",
    "\n",
    "In the validation schema, users only specify the validators that they would like to use. This abstracts away details concerning what methods should be used for which columns and simplifies the process for searching for a particular flavor of issues to a few lines of code. Below is the validation schema that we will use for the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fbdc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_schema : Dict = {\n",
    "    \"default_inclusion\": False,\n",
    "    \"validators\": {\n",
    "        \"unsupervised_anomaly_data_validator\": {},\n",
    "        \"unsupervised_multimodal_anomaly_data_validator\": {},\n",
    "        \"split_covariate_data_validator\": {},\n",
    "        \"ood_inference_data_validator\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5ebfa",
   "metadata": {},
   "source": [
    "A few notes: \n",
    "- *default inclusion* referes to whether the validation should include default validators (validators with `is_default` set to `True`)\n",
    "- Each validator maps to an object that specifies additional options. For this tutorial, we will use the default settings, but these options include: \n",
    "  - special kwargs to include and pass to the validator methods\n",
    "  - additional filtering regarding which columns the validator should be applied to\n",
    " \n",
    "  \n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f89af",
   "metadata": {},
   "source": [
    "## Specifying Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1525d",
   "metadata": {},
   "source": [
    "It may be the case that you are using a data modality that has little to no method infrastructure in Data Detective. The simplest way to make use of all of Data Detective's functionality is to use a transform that maps this data modality to a well-supported modality in Data Detective such as multidimensional data. In our example, we will be making use of a pretrained resnet50 backbone to map our MNIST images to 2048 dimensional vectors. This will allow us to make use of methods built for multidimensional data on our MNIST image representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_schema : Dict = {\n",
    "    \"transforms\": {\n",
    "        \"image\": [{\n",
    "            \"name\": \"resnet50\",\n",
    "            \"in_place\": \"False\",\n",
    "            \"options\": {},\n",
    "        }],\n",
    "    }\n",
    "}\n",
    "     \n",
    "full_validation_schema: Dict = {\n",
    "    **validation_schema, \n",
    "    **transform_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3611d3",
   "metadata": {},
   "source": [
    "# Running the Data Detective Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecb162",
   "metadata": {},
   "source": [
    "Now that the full validation schema and data object are prepared, we are ready to run the Data Detective Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692ee2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running validator class unsupervised_anomaly_data_validator...\n",
      "running validator class unsupervised_multimodal_anomaly_data_validator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window(order, start, length):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/links.py:5: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def identity(x):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/links.py:10: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _identity_inverse(x):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/links.py:15: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def logit(x):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/links.py:20: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _logit_inverse(x):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/mcconnl3/Code/data-detective-load-test/dd_env/lib/python3.9/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running validator class split_covariate_data_validator...\n",
      "running validator class ood_inference_data_validator...\n",
      "thread 11393855488 entered to handle validator method iforest_anomaly_validator_methodthread 11410681856 entered to handle validator method cblof_anomaly_validator_method\n",
      "thread 11410681856:    running cblof_anomaly_validator_method...\n",
      "\n",
      "thread 11393855488:    running iforest_anomaly_validator_method...\n",
      "thread 11427508224 entered to handle validator method pca_anomaly_validator_method\n",
      "thread 11427508224:    running pca_anomaly_validator_method...\n",
      "thread 11444334592 entered to handle validator method iforest_multimodal_anomaly_validator_method\n",
      "thread 11444334592:    running iforest_multimodal_anomaly_validator_method...\n",
      "thread 11461160960 entered to handle validator method pca_multimodal_anomaly_validator_method\n",
      "thread 11461160960:    running pca_multimodal_anomaly_validator_method...\n",
      "thread 11477987328 entered to handle validator method cblof_multimodal_anomaly_validator_method\n",
      "thread 11477987328:    running cblof_multimodal_anomaly_validator_method...\n",
      "thread 11494813696 entered to handle validator method mann_whitney_multidimensional_split_validator_method\n",
      "thread 11494813696:    running mann_whitney_multidimensional_split_validator_method...\n",
      "thread 11511640064 entered to handle validator method mann_whitney_split_validator_method\n",
      "thread 11511640064:    running mann_whitney_split_validator_method...\n",
      "thread 11528466432 entered to handle validator method kolmogorov_smirnov_split_validator_method\n",
      "thread 11528466432:    running kolmogorov_smirnov_split_validator_method...\n",
      "thread 11545292800 entered to handle validator method kruskal_wallis_multidimensional_split_validator_method\n",
      "thread 11545292800:    running kruskal_wallis_multidimensional_split_validator_method...\n",
      "thread 11562119168 entered to handle validator method kolmogorov_smirnov_multidimensional_split_validator_method\n",
      "thread 11562119168:    running kolmogorov_smirnov_multidimensional_split_validator_method...\n",
      "thread 11578945536 entered to handle validator method kruskal_wallis_split_validator_method\n",
      "thread 11578945536:    running kruskal_wallis_split_validator_method...\n",
      "thread 11595771904 entered to handle validator method pca_anomaly_validator_method\n",
      "thread 11595771904:    running pca_anomaly_validator_method...\n",
      "thread 11612598272 entered to handle validator method cblof_anomaly_validator_method\n",
      "thread 11612598272:    running cblof_anomaly_validator_method...\n",
      "thread 11629424640 entered to handle validator method iforest_anomaly_validator_method\n",
      "thread 11629424640:    running iforest_anomaly_validator_method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 11427508224: finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 11461160960: finished\n",
      "thread 11511640064: finished\n",
      "thread 11528466432: finished\n",
      "thread 11578945536: finished\n",
      "thread 11595771904: finished\n",
      "thread 11444334592: finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 11629424640: finished\n",
      "thread 11393855488: finished\n",
      "thread 11612598272: finished\n",
      "thread 11477987328: finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 11562119168: finished\n",
      "thread 11410681856: finished\n",
      "thread 11494813696: finished\n",
      "thread 11545292800: finished\n",
      "--- 15.769446849822998 seconds ---\n"
     ]
    }
   ],
   "source": [
    "data_detective_engine = DataDetectiveEngine()\n",
    "\n",
    "# 1 thread, --- 220.85648322105408 seconds ---\n",
    "# multithreadinng (joblib), --- 149.11400604248047 seconds ---\n",
    "# thread pools, --- 81.38025784492493 seconds ---\n",
    "# data-level caching, clean cache, --- 75.22503590583801 seconds ---\n",
    "# sample-level caching, clean cache--- 26.184876918792725 seconds ---\n",
    "# data-level caching, dirty cache, --- 22.925609827041626 seconds ---\n",
    "# sample-level caching, dirty cache, --- 19.73765206336975 seconds ---\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "results = data_detective_engine.validate_from_schema(full_validation_schema, data_object)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e1d58",
   "metadata": {},
   "source": [
    "Great! Let's start to look at and analyze the results we've collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84578eae",
   "metadata": {},
   "source": [
    "# Interpreting Results using the Built-In Rank Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show_datapoint(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
