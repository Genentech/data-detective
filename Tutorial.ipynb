{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961c370f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d45ce",
   "metadata": {},
   "source": [
    "In this tutorial, we will explain the basics of using Data Detective to perform a Data Investigation. In this tutorial, we will go through the steps of setting up and running a basic Data Detective Investigation, including: \n",
    "1. Configuring a dataset that works with Data Detective. \n",
    "2. Setting up a schema that defines the Data Detective investigation. \n",
    "3. Executing a data detective investigation. \n",
    "4. Summarizing results using the built-in Rank Aggregator\n",
    "\n",
    "\n",
    "We will also include supplemental tutorials for some of Data Detective's more advanced features, including: \n",
    "- Extending the Data Detective investigation with custom validations\n",
    "- Extending the transform library to map custom datatypes to supported datatypes\n",
    "\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1139eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from typing import Dict, Union\n",
    "\n",
    "from constants import FloatTensor\n",
    "from src.aggregation.rankings import RankingAggregator, RankingAggregationMethod\n",
    "from src.data_detective_engine import DataDetectiveEngine\n",
    "from src.datasets.tutorial_dataset import TutorialDataset\n",
    "from src.enums.enums import DataType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfa30b",
   "metadata": {},
   "source": [
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028623e",
   "metadata": {},
   "source": [
    "## Requirements for a Dataset\n",
    "\n",
    "For a dataset to work within the Data Detective framework, it needs to satisfy the following requirements: \n",
    "\n",
    "1. It must override the `__getitem__` method that returns a dictionary mapping from each data column key to the data value. \n",
    "2. It must contain a `datatypes` method that returns a dictionary mapping from each data column key to the column's datatype. \n",
    "\n",
    "Let's examine what this looks like in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87d144",
   "metadata": {},
   "source": [
    "## Dataset Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd5898",
   "metadata": {},
   "source": [
    "In this tutorial, we will create a heterogeneous dataset that consists of the following items: \n",
    "\n",
    "- MNIST images\n",
    "- MNIST labels\n",
    "- 10-dimensional normal distribution (μ=0, σ=1)\n",
    "\n",
    "The full dataset can be found under src/datasets/tutorial_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TutorialDataset(\n",
    "    root='./data/MNIST',\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor() \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7faeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show_datapoint(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a420b",
   "metadata": {},
   "source": [
    "# Setting up the Data Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87def2d",
   "metadata": {},
   "source": [
    "The *data object* consists of the preprocessed dataset and (optionally) its splits. In order to make use of split validation techniques such as distribution shift between splits, the data splits must be included. \n",
    "\n",
    "For the purpose of thsi tutorial, we are going to hold out 20 samples that we are going to use to model \"real world\" samples that will be encountered at inference time. We will then perform a 60/20/20 split between train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_size: int = 20\n",
    "everything_but_inference_size: int = dataset.__len__() - inference_size\n",
    "inference_dataset, everything_but_inference_dataset = torch.utils.data.random_split(dataset, [inference_size, dataset.__len__() - inference_size])\n",
    "    \n",
    "train_size: int = int(0.6 * len(everything_but_inference_dataset))\n",
    "val_size: int = int(0.2 * len(everything_but_inference_dataset))\n",
    "test_size: int = len(everything_but_inference_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(everything_but_inference_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "data_object: Dict[str, torch.utils.data.Dataset] = {\n",
    "    \"training_set\": train_dataset,\n",
    "    \"validation_set\": val_dataset,\n",
    "    \"test_set\": test_dataset,\n",
    "    \"entire_set\": dataset,\n",
    "    \"everything_but_inference_set\": everything_but_inference_dataset,\n",
    "    \"inference_set\": inference_dataset\n",
    "}\n",
    "\n",
    "print(f\"size of inference_dataset: {inference_dataset.__len__()}\")\n",
    "print(f\"size of everything_but_inference_dataset: {everything_but_inference_dataset.__len__()}\")\n",
    "print(f\"size of train_dataset: {train_dataset.__len__()}\")\n",
    "print(f\"size of entire dataset: {dataset.__len__()}\")\n",
    "print(f\"size of val_dataset: {val_dataset.__len__()}\")\n",
    "print(f\"size of test_dataset: {test_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2197dc",
   "metadata": {},
   "source": [
    "# Setting up a Validation Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be22c1",
   "metadata": {},
   "source": [
    "## Specifying Validators and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033c9e0",
   "metadata": {},
   "source": [
    "The validation schema contains information about the types of checks that will be executed by the Data Detective Engine and the transforms that Data Detective will use. Before discussing the validation schema, it is important to define two key modular components that make up Data Detective investigations. Data Detective's functionality can be divided into a modular, implementation-heavy component referred to as a *validator method* and an easily toggleable component referred to as a *validator*.\n",
    "\n",
    "A *validator method* performs a specific type of test for a data issue on a specific data type. These validator methods primarily consist of the code needed to take a dataset and run an evaluation on it that produces either a positive or negative result or a score. Some examples of validator methods include:\n",
    "- Mann-Whitney U-Test to examine distribution shift between train/test splits on tabular data\n",
    "- Kernel Conditional Independence (KCI) test for validating causal assumptions on vectorvalued\n",
    "data\n",
    "- Isolation forest trained/evaluated on image histograms for identifying anomalies in imaging\n",
    "data\n",
    "\n",
    "*Validations* are toggleable, datatype-agnostic collections of validator methods that are serially applied to the dataset. Each validator targets a single problem that may arise in data, including: \n",
    "- Shift between different data splits\n",
    "- Outlier and anomaly detection\n",
    "- Violation of parametric assumptions on the data\n",
    "- Violation of expected casual structures / conditional independences in the \n",
    "\n",
    "In the validation schema, users only specify the validators that they would like to use. This abstracts away details concerning what methods should be used for which columns and simplifies the process for searching for a particular flavor of issues to a few lines of code. Below is the validation schema that we will use for the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_schema : Dict = {\n",
    "    \"default_inclusion\": False,\n",
    "    \"validators\": {\n",
    "        \"unsupervised_anomaly_data_validator\": {},\n",
    "        \"unsupervised_multimodal_anomaly_data_validator\": {},\n",
    "        \"split_covariate_data_validator\": {},\n",
    "        \"ood_inference_data_validator\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5ebfa",
   "metadata": {},
   "source": [
    "A few notes: \n",
    "- *default inclusion* referes to whether the validation should include default validators (validators with `is_default` set to `True`)\n",
    "- Each validator maps to an object that specifies additional options. For this tutorial, we will use the default settings, but these options include: \n",
    "  - special kwargs to include and pass to the validator methods\n",
    "  - additional filtering regarding which columns the validator should be applied to\n",
    " \n",
    "  \n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f89af",
   "metadata": {},
   "source": [
    "## Specifying Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1525d",
   "metadata": {},
   "source": [
    "It may be the case that you are using a data modality that has little to no method infrastructure in Data Detective. The simplest way to make use of all of Data Detective's functionality is to use a transform that maps this data modality to a well-supported modality in Data Detective such as multidimensional data. In our example, we will be making use of a pretrained resnet50 backbone to map our MNIST images to 2048 dimensional vectors. This will allow us to make use of methods built for multidimensional data on our MNIST image representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_schema : Dict = {\n",
    "    \"transforms\": {\n",
    "        \"image\": [{\n",
    "            \"name\": \"resnet50\",\n",
    "            \"in_place\": \"False\",\n",
    "            \"options\": {},\n",
    "        }],\n",
    "    }\n",
    "}\n",
    "     \n",
    "full_validation_schema: Dict = {\n",
    "    **validation_schema, \n",
    "    **transform_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3611d3",
   "metadata": {},
   "source": [
    "# Running the Data Detective Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecb162",
   "metadata": {},
   "source": [
    "Now that the full validation schema and data object are prepared, we are ready to run the Data Detective Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ee2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_detective_engine = DataDetectiveEngine()\n",
    "\n",
    "# 1 thread, --- 220.85648322105408 seconds ---\n",
    "# multithreadinng (joblib), --- 149.11400604248047 seconds ---\n",
    "# thread pools, --- 81.38025784492493 seconds ---\n",
    "# data-level caching, clean cache, --- 75.22503590583801 seconds ---\n",
    "# sample-level caching, clean cache--- 26.184876918792725 seconds ---\n",
    "# data-level caching, dirty cache, --- 22.925609827041626 seconds ---\n",
    "# sample-level caching, dirty cache, --- 19.73765206336975 seconds ---\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "results = data_detective_engine.validate_from_schema(full_validation_schema, data_object)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e1d58",
   "metadata": {},
   "source": [
    "Great! Let's start to look at and analyze the results we've collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84578eae",
   "metadata": {},
   "source": [
    "# Interpreting Results using the Built-In Rank Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show_datapoint(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
